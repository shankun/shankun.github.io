<!doctype html><html lang=zh_CN><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="light dark" name=color-scheme><title>Philosophy of Coroutines</title><link href=https://shankun.github.io/img/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=https://shankun.github.io/img/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=https://shankun.github.io/img/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><style>body{--primary-color:#5871a2;--primary-pale-color:#5871a210;--text-color:#3c4043;--text-pale-color:#94969f;--bg-color:#fff;--highlight-mark-color:#5f75b035;--callout-note-color:#5871a2;--callout-important-color:#8062b0;--callout-warning-color:#936e51;--callout-alert-color:#bc5252;--callout-question-color:#477389;--callout-tip-color:#3c8460;--main-font:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--code-font:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--homepage-max-width:768px;--main-max-width:768px;--avatar-size:60px;--icon-size:20px;--homepage-font-size:16px;--homepage-line-height:1.75;--paragraph-font-size:16px;--paragraph-line-height:1.75;--aside-font-size:15px;--img-border-radius:0;--callout-border-radius:0;--detail-border-radius:0;--dark-mode-img-brightness:.75;--dark-mode-chart-brightness:.75;--inline-code-border-radius:2px;--inline-code-bg-color:var(--primary-pale-color);--block-code-border-radius:0;--block-code-border-color:var(--primary-color);--detail-border-color:var(--primary-color);--homepage-bg-image:url(https://shankun.github.io/img/greenBamboo.jpg)}body.dark{--primary-color:#5d77ac;--primary-pale-color:#5d77ac20;--text-color:#9197a5;--text-pale-color:#747983;--bg-color:#202124;--highlight-mark-color:#5f75b035;--callout-note-color:#5d77ac;--callout-important-color:#8062b0;--callout-warning-color:#936e51;--callout-alert-color:#bc5252;--callout-question-color:#477389;--callout-tip-color:#3c8460}</style><link href=https://shankun.github.io/main.css rel=stylesheet><link href=https://shankun.github.io/hl-light.css id=hl rel=stylesheet><link crossorigin href=https://fastly.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV rel=stylesheet><script crossorigin defer integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 src=https://fastly.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js></script><script crossorigin defer integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 src=https://fastly.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}],throwOnError:false})})</script><body class=post><script>const theme = sessionStorage.getItem('theme');
    const match = window.matchMedia("(prefers-color-scheme: dark)").matches
    if ((theme && theme == 'dark') || (!theme && match)) {
      document.body.classList.add('dark');
      const hl = document.querySelector('link#hl');
      if (hl) hl.href = https://shankun.github.io/hl-dark.css;
    }</script><header class=blur><div id=header-wrapper><nav><a class=instant href=https://shankun.github.io>ÊØè‰∏ÄÂ§©ÈÉΩÊòØÁ¶ªÂà´</a><button aria-label="toggle expand" class=separator id=toggler>üöÄ</button><span class="wrap left fold">{</span><a class=instant href=https://shankun.github.io/posts>articles</a><span class="wrap-separator fold">,</span><a class="instant fold" href=https://shankun.github.io/projects>projects</a><span class="wrap right fold">} ;</span></nav><div id=btns><button aria-label="theme switch" data-moon-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill="currentColor"></path></svg>' data-sun-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M12 18C8.68629 18 6 15.3137 6 12C6 8.68629 8.68629 6 12 6C15.3137 6 18 8.68629 18 12C18 15.3137 15.3137 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM11 1H13V4H11V1ZM11 20H13V23H11V20ZM3.51472 4.92893L4.92893 3.51472L7.05025 5.63604L5.63604 7.05025L3.51472 4.92893ZM16.9497 18.364L18.364 16.9497L20.4853 19.0711L19.0711 20.4853L16.9497 18.364ZM19.0711 3.51472L20.4853 4.92893L18.364 7.05025L16.9497 5.63604L19.0711 3.51472ZM5.63604 16.9497L7.05025 18.364L4.92893 20.4853L3.51472 19.0711L5.63604 16.9497ZM23 11V13H20V11H23ZM4 11V13H1V11H4Z" fill="currentColor"></path></svg>' id=theme-toggle><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill=currentColor></path></svg></button><button aria-label="table of content" id=toc-toggle><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M3 4H21V6H3V4ZM3 11H15V13H3V11ZM3 18H21V20H3V18Z" fill=currentColor></path></svg></button></div></div></header><div id=wrapper><div id=blank></div><aside><nav><ul><li><a class=h2 href=#intro>Introduction</a><li><a class=h2 href=#awesome>Why I‚Äôm so enthusiastic about coroutines</a> <ul><li><a class=h3 href=#objective>The objective view: what makes them useful?</a><li><a class=h3 href=#subjective>The subjective view: why do I like them so much?</a></ul><li><a class=h2 href=#technique>Techniques for getting the most out of coroutines</a> <ul><li><a class=h3 href=#when>When to use coroutines, and when not to</a><li><a class=h3 href=#use-cases>Types of code that might usefully become coroutines</a><li><a class=h3 href=#large-and-small>Coroutines large and small</a><li><a class=h3 href=#queues>Combine with input queues</a><li><a class=h3 href=#handlers>Combine with ambient pre-filters</a></ul><li><a class=h2 href=#features>Coroutine paradigms</a> <ul><li><a class=h3 href=#taocp>TAOCP‚Äôs coroutines: symmetric, utterly stackless</a><li><a class=h3 href=#resumable-callee>A subroutine that can resume from where it last left off</a><li><a class=h3 href=#cothreads>Cooperative threads that identify which thread to transfer to next</a><li><a class=h3 href=#named-object>A named object identifying a program activity</a></ul><li><a class=h2 href=#conclusion>Conclusion</a><li><a class=h2 href=#footnotes>Footnotes</a></ul></nav><button aria-label="back to top" id=back-to-top><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M11.9997 10.8284L7.04996 15.7782L5.63574 14.364L11.9997 8L18.3637 14.364L16.9495 15.7782L11.9997 10.8284Z" fill=currentColor></path></svg></button></aside><main><div><div data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>' id=copy-cfg style=display:none></div><article data-backlink-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M9.41421 8L18.0208 16.6066L16.6066 18.0208L8 9.41421V17H6V6H17V8H9.41421Z" fill="currentColor"></path></svg>' class=prose><h1>Philosophy of Coroutines</h1><div id=post-info><div id=date><span id=publish>2023-09-01</span></div><div id=tags><a class=instant href=https://shankun.github.io/tags/coroutines><span>#</span>coroutines</a><a class=instant href=https://shankun.github.io/tags/c><span>#</span>C++</a></div></div><ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#intro>Introduction</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#awesome>Why I‚Äôm so enthusiastic about coroutines</a> <ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#objective>The objective view: what makes them useful?</a> <ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#vs-state-machines>Versus explicit state machines</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#vs-threads>Versus conventional threads</a></ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#subjective>The subjective view: why do <em>I</em> like them so much?</a> <ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#student-ready>‚ÄúTeach the student when the student is ready‚Äù</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#clarity>They suit my particular idea of code clarity</a></ul></ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#technique>Techniques for getting the most out of coroutines</a> <ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#when>When to use coroutines, and when not to</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#use-cases>Types of code that might usefully become coroutines</a> <ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#generator>Output only: generators</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#consumer>Input only: consumers</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#adapter>Separate input and output: adapters</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#protocol>Input and output talking to the same entity: protocols</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#uc-general>More general, and miscellaneous</a></ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#large-and-small>Coroutines large and small</a> <ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#activation-energy>Activation energy</a></ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#queues>Combine with input queues</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#handlers>Combine with ambient pre-filters</a></ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#features>Coroutine paradigms</a> <ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#taocp>TAOCP‚Äôs coroutines: symmetric, utterly stackless</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#resumable-callee>A subroutine that can resume from where it last left off</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#cothreads>Cooperative threads that identify which thread to transfer to next</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#named-object>A named object identifying a program activity</a></ul><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#conclusion>Conclusion</a><li><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#footnotes>Footnotes</a></ul><h2 id=intro>Introduction<a aria-label="Anchor link for: intro" class=zola-anchor href=#intro style=visibility:hidden>#</a></h2><p>I‚Äôve been a huge fan of coroutines since the mid-1990s.<p>I encountered the idea as a student, when I first read The Art of Computer Programming. I‚Äôd been programming already for most of my childhood, and I was completely blown away by the idea, which was entirely new to me. In fact, it‚Äôs probably not an exaggeration to say that in the whole of TAOCP that was the single thing that changed my life the most.<p>Within a few months of encountering the idea for the first time, I found myself wanting to use it in real code, during a vacation job at a (now defunct) tech company. Sadly I was coding in C at the time, which didn‚Äôt support coroutines. But I didn‚Äôt let that stop me: I came up with a C preprocessor trick that worked well enough to fake them, and did it anyway.<p>The trick I came up with has limitations, but it‚Äôs reliable enough to use in serious code. I wrote it up a few years later in the article ‚Äò<a rel="nofollow noreferrer" href=https://www.chiark.greenend.org.uk/~sgtatham/coroutines.html>Coroutines in C</a>‚Äô, and I‚Äôve been using it ever since, in both C and C++, whenever the code I‚Äôm trying to write looks as if it would be most naturally expressed in that style.<p>Or rather, I do that in code bases <em>where I can get away with it</em>, because it‚Äôs unquestionably a weird and unorthodox style of C by normal standards, and not everyone is prepared to take it in their stride. Mostly I limit my use of C-preprocessor coroutines to free software projects where I‚Äôm the lead or only developer (in particular, <a rel="nofollow noreferrer" href=https://www.chiark.greenend.org.uk/~sgtatham/putty/>PuTTY</a> and <a rel="nofollow noreferrer" href=https://www.chiark.greenend.org.uk/~sgtatham/spigot/><code>spigot</code></a>), because there, I get to choose what‚Äôs an acceptable coding style and what‚Äôs not. The only time I‚Äôve ever used the same technique in code I wrote for an employer was in that original vacation job where I invented the trick ‚Äì and I‚Äôm sure I only got away with it that time because my team didn‚Äôt practise code review.<p>Of course, I‚Äôm also happy to use coroutine-style idioms in any language where you <em>don‚Äôt</em> have to resort to trickery, such as using generators in Python whenever it seems like a good idea.<p>Whenever I <em>can‚Äôt</em> use coroutines, I feel limited, because it‚Äôs become second nature to me to spot parts of a program that <em>want</em> to be written in coroutine style, and would be more awkward to express in other ways. So I‚Äôm very much in favour of coroutines becoming more mainstream ‚Äì and I‚Äôve been pleased to see support appearing in more and more languages over the decades. At the time of writing this, Wikipedia has an <a rel="nofollow noreferrer" href=https://en.wikipedia.org/wiki/Coroutine#Native_support>impressive list of languages</a> that now include coroutine support; from my own perspective, the big ones are Python generators, and the addition of a coroutine system to C++20.<p>I recently got round to <a rel="nofollow noreferrer" href=https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/coroutines-c++20/>learning about the C++20 system</a> in detail. In the course of that, I had conversations about coroutines with several friends and colleagues, which made me think more seriously about the fact that I‚Äôm so fond of them, and wonder if I had anything more specific to say about that than ‚Äòthey‚Äôre really useful‚Äô.<p>It turned out that I did. So this article presents my ‚Äòphilosophy of coroutines‚Äô: <em>why</em> I find them so useful, what kinds of thing they‚Äôre useful for, tricks I‚Äôve learned for getting the most out of them, and even a few different ways of thinking <em>about</em> them.<p>But it discusses coroutines as a general concept, not specific to their implementation in any particular language. So where I present code snippets, they‚Äôll be in whatever language is convenient to the example, or even in pure pseudocode.<h2 id=awesome>Why I‚Äôm so enthusiastic about coroutines<a aria-label="Anchor link for: awesome" class=zola-anchor href=#awesome style=visibility:hidden>#</a></h2><p>Go on then, why <em>do</em> I like coroutines so much?<p>Well, from first principles, there are two possibilities. Either it‚Äôs because they really are awesome, or it‚Äôs because they suit my personal taste. (Or a combination of the two, of course.)<p>I can think of arguments for both of those positions, but I don‚Äôt think I have the self-awareness to decide which is more significant. So I‚Äôll just present both sets of arguments, and let you make up your own mind about whether you end up also thinking they‚Äôre awesome.<h3 id=objective>The objective view: what makes them useful?<a aria-label="Anchor link for: objective" class=zola-anchor href=#objective style=visibility:hidden>#</a></h3><p>To answer ‚Äòwhat makes coroutines useful?‚Äô, it‚Äôs useful to start by deciding what we‚Äôre <em>comparing</em> them to. I can‚Äôt answer ‚Äòwhy is a coroutine more useful than <em>X?</em>‚Äô without knowing what <em>X</em> we‚Äôre talking about. And there‚Äôs more than one choice.<h4 id=vs-state-machines>Versus explicit state machines<a aria-label="Anchor link for: vs-state-machines" class=zola-anchor href=#vs-state-machines style=visibility:hidden>#</a></h4><p>Two decades ago, when I wrote ‚Äò<a rel="nofollow noreferrer" href=https://www.chiark.greenend.org.uk/~sgtatham/coroutines.html>Coroutines in C</a>‚Äô, I included an explanation of why coroutines are useful, based on the idea that the alternative was to write an ordinary function containing an explicit state machine.<p>Suppose you have a function that consumes a stream of values, and due to the structure of the rest of your program, it has to receive each individual value via a separate function call:<pre class=z-code><code><span class="z-text z-plain">function process_next_value(v) {
    // ...
}
</span></code></pre><p>And suppose the nature of the stream is such that the way you handle each input value depends on what the previous values were. I‚Äôll show an example similar to the previous article, which is a simple run-length decompressor processing a stream of input bytes: the idea is that most input bytes are emitted literally to the output, but one byte value is special, and is followed by a (length, output byte) pair indicating that the output byte should be repeated a number of times.<p>To write that as a conventional function accepting a single byte on each call, you have to keep a state variable of some kind to remind you of what you were going to use the next byte for:<pre class=z-code><code><span class="z-text z-plain">function process_next_byte(byte) {
    if (state == TOP_LEVEL) {
        if (byte != ESCAPE_BYTE) {
            // emit the byte literally and stay in the same state
            output(byte);
        } else {
            // go into a state where we expect to see a run length next
            state = EXPECT_RUN_LENGTH;
        }
    } else if (state == EXPECT_RUN_LENGTH) {
        // store the length
        run_length = byte;
        // go into a state where we expect the byte to be repeatedly output
        state = EXPECT_OUTPUT;
    } else if (state == EXPECT_OUTPUT) {
        // output this byte the right number of times
        for i in (1,...,run_length)
            output(byte);
        // and go back to the top-level state for the next input byte
        state = TOP_LEVEL;
    }
}
</span></code></pre><p>(This is pseudocode, so I‚Äôm not showing the details of how you arrange for <code>state</code> and <code>run_length</code> to be preserved between calls to the function. Whatever‚Äôs convenient in your language. These days they‚Äôre probably member variables of a class, in most languages.)<p>This is pretty unwieldy code to write, because structurally, it looks a lot like a collection of blocks of code connected by ‚Äògoto‚Äô statements. They‚Äôre not <em>literally</em> goto statements ‚Äì each one works by setting <code>state</code> to some value and returning from the function ‚Äì but they have the same semantic effect, of stating by name which part of the function is going to be run next.<p>The more complicated the control structure is, the more cumbersome it becomes to write code in this style, or to read it. Suppose I wanted to enhance my run-length compression format so that it could efficiently represent repetitions of a <em>sequence</em> of bytes, not just <code>AAAAAAA</code> but <code>ABABABAB</code> or <code>ABCDEABCDE</code>? Then I wouldn‚Äôt just need an <em>output</em> loop to emit <em>n</em> copies of something; I‚Äôd also need a loop on the input side, because the input format would probably contain an ‚Äòinput sequence length‚Äô byte followed by that many bytes of literal input. But I can‚Äôt write that one as a <code>for</code> statement, because it has to return to the caller to get each input byte; instead, I‚Äôd have to build a ‚Äòfor loop‚Äô structure out of these small building blocks.<p>The more code you write in this style, the more you probably wish that the caller and callee were the other way round. If you were writing the same run-length decompression code in a context where you <em>called</em> a function to read the next byte, it would look a lot more natural (not to mention shorter), because you could call the get-byte function in multiple places:<pre class=z-code><code><span class="z-text z-plain">function run_length_decompress(stream) {
    while (byte = stream.getbyte()) {
        if (byte != ESCAPE_BYTE) {
            output(byte);
        } else {
            run_length = stream.getbyte();
            byte_to_repeat = stream.getbyte();
            for i in (1,...,run_length)
                output(byte_to_repeat);
        }
    }
}
</span></code></pre><p>This function actually contains the same state machine as the previous version ‚Äì but it‚Äôs entirely implicit. If you want to know what each piece of code will do when its next input byte arrives (say, during debugging), then in the state-machine version you answer it by asking for the value of <code>state</code> ‚Äì but in this version, you‚Äôd answer it by looking at a stack backtrace to find out which of the calls to <code>stream.getbyte()</code> the byte will be returned from, because that‚Äôs what controls where execution will resume from.<p>In other words, each of the three state values <code>TOP_LEVEL</code>, <code>EXPECT_RUN_LENGTH</code> and <code>EXPECT_OUTPUT</code> in the state-machine code corresponds precisely to one of the three calls to <code>stream.getbyte()</code> in this version. So there‚Äôs still a piece of data in the program tracking which of those ‚Äòstates‚Äô we‚Äôre in ‚Äì but it‚Äôs the <em>program counter</em>, and doesn‚Äôt need to be a named variable with a set of explicit named or numbered state values.<p>And if you needed to extend <em>this</em> version of the code so that it could read a list of input bytes to repeat, it would be no trouble at all, because there‚Äôs nothing to stop you putting a call to <code>stream.getbyte()</code> in a loop:<pre class=z-code><code><span class="z-text z-plain">            num_output_repetitions = stream.getbyte();
            num_input_bytes = stream.getbyte();
            for i in (1,...,num_input_bytes)
                input_bytes.append(stream.getbyte());
            for i in (1,...,num_output_repetitions)
                for byte in input_bytes
                    output(byte);

</span></code></pre><p>and you wouldn‚Äôt have to mess around with inventing extra values in your state enumeration, coming up with names for them, checking the names weren‚Äôt already used, renaming other states if their names had become ambiguous, carefully writing transitions to and from existing states, etc. You can just casually write a loop, and the programming language takes care of the rest.<p>(I tend to think of the state-machine style as ‚Äòturning the function inside out‚Äô, in a more or less literal sense: the calls to a ‚Äòget byte‚Äô function ought to be nested deeply <em>inside</em> the code‚Äôs control flow, but instead, we‚Äôre forced to make them the <em>outermost</em> layer, so that ‚Äònow wait for another byte‚Äô consists of control falling off the end of the function, and ‚Äòok, we‚Äôve got one now, carry on‚Äô is at the top.)<p>Using coroutines, you can write the code in this latter style, with the explicit state machine replaced by the implicit program counter, <em>even</em> if the rest of the program needs to work by passing each output byte to a function call.<p>In other words, you can write this part of the program ‚Äòthe way it wants to be written‚Äô ‚Äì the way that is most convenient for the nature of what the code has to do. And you didn‚Äôt have to force the <em>rest</em> of the program to change its structure to compensate. That might have required equally awkward contortions elsewhere, in which case you‚Äôd only have moved the awkwardness around, and not removed it completely.<p>Coroutines give you greater freedom to choose the structure of each part of your program, independently of the other parts. So each part can be as clear as possible.<p><em>Coroutines mean never having to turn your code inside out.</em><h4 id=vs-threads>Versus conventional threads<a aria-label="Anchor link for: vs-threads" class=zola-anchor href=#vs-threads style=visibility:hidden>#</a></h4><p>In the 1990s and early 2000s, that was the only argument I felt I needed in favour of coroutines. But now it‚Äôs 2023, and coroutines have another competitor: multithreading.<p>Another way to solve the code-structure dilemma in the previous section would be to put the decompression code into its own thread. You‚Äôd have to set up a thread-safe method of giving it its input values ‚Äì say, some kind of lock-protected queue or ring buffer, which would cause the decompressor thread to block if the queue was empty, and the thread providing the data to block if it was full. But then there would be no reason to <em>pretend</em> either side was making a function call when it really wasn‚Äôt. Each side <em>really would</em> be making a function call to provide an output byte or read an input byte, and there would be no contradiction, because the call stacks of the two threads are completely independent.<p>One of my software projects, <a rel="nofollow noreferrer" href=https://www.chiark.greenend.org.uk/~sgtatham/spigot/><code>spigot</code></a>, is a complicated C++ program completely full of coroutines that generate streams of data. I was chatting to a friend recently about the possibility ‚Äì in principle ‚Äì of rewriting it in Rust. But I don‚Äôt actually know a lot about Rust yet (if I did try this, it would be a learning experience and a half!), and so of course I asked: ‚ÄòWhat about all the coroutines? Would I implement those using async Rust in some way, or what?‚Äô<p>My Rust-programmer friend replied: no, don‚Äôt even bother. Just use threads. Turn each of <code>spigot</code>‚Äôs coroutines into ordinary non-async Rust code, and run it in its own thread, passing its output values to some other thread which will block until a value is available. <em>It‚Äôll be fine. Threads are cheap.</em><p>I have to start by acknowledging my personal biases: my <em>first</em> reaction to that attitude is purely emotional. It just gives me the heebie-jeebies.<p>My own programming style has always been single-threaded wherever possible. I mostly only use extra threads when I really can‚Äôt avoid it, usually because some operating system or library API makes it impossible, or <em>astonishingly</em> awkward, to do a thing any other way.<sup id=footnote-win32-read-write>1</sup> I‚Äôve occasionally gone as far as parallelising a large number of completely independent computations using some convenient ready-made system like Python <code>multiprocessing</code>, but I‚Äôm at least as likely to do that kind of thing by putting each computation into a completely separate <em>process</em>, and parallelising them at the process level, using a <code>make</code>-type tool or GNU <code>parallel</code>.<p>Partly, that‚Äôs because I still think of threads as heavyweight. On Linux, they consume a process id, and those have a limited supply. Switching between threads requires a lot more saving and restoring of registers than an ordinary function call, and <em>also</em> needs a transfer from user mode to kernel mode and back, with an extra set of saves and restores for that. The data structures you use to transfer data values between the threads must be protected by a locking mechanism, which costs extra time and faff. <em>Perhaps</em> all of these considerations are minor in 2023, where they were significant in 2000? I still <em>feel</em> as if I wouldn‚Äôt want to commit to casually making 250 threads in the course of a single <code>spigot</code> computation (which is about the number of coroutines it constructs in the course of computing a few hundred digits of <a rel="nofollow noreferrer" href=https://xkcd.com/217/>$e^œÄ‚àíœÄ$</a>), <em>or</em> to doing a kernel-level thread switch for every single time one of those threads passes a tuple of four integers to another. <code>spigot</code> is slow enough as it is ‚Äì and one of my future ambitions for it is to be able to parallelise 1000 of <em>those</em> computations! But I have to admit that I haven‚Äôt actually tried this strategy and benchmarked it. So <em>maybe</em> I have an outdated idea of what it would cost.<p>Also, I think of threads as <em>dangerous</em>. You have to do all that locking and synchronisation to prevent race conditions, and it‚Äôs often complicated, and easy to get wrong. My instinct is to view any non-trivial use of threads as 100 data-race bugs waiting to happen. (And if you manage to avoid all of those, probably half a dozen deadlocks hiding behind them!) Now in this <em>particular</em> conversation we were talking about Rust, and that‚Äôs a special case on this count, because Rust‚Äôs unique selling point is to guarantee that your program is free of data races by the time you‚Äôve managed to get it to compile at all. But in more or less any other language I think I‚Äôm still right to be scared!<p>So I have strong prejudices against ‚Äòjust use real threads, it‚Äôll be fine‚Äô. But I accept that I <em>might</em> be wrong about those. Supposing I am, are there any other arguments for using coroutines instead?<p>One nice feature of everything being in the same system thread is debuggability. If you‚Äôre using an interactive debugger, you don‚Äôt have to worry about which thread you‚Äôre debugging, or which thread a breakpoint applies to: if the whole overall computation is happening in the same thread then you can just place a breakpoint in the normal way and it will be hit.<p>And if you prefer debugging via ‚Äòrecompile with print statements‚Äô, that‚Äôs <em>also</em> more convenient in a single-threaded setup where the transfers of control are explicit, because you don‚Äôt have to worry about adding extra locks to ensure the debug messages themselves are atomic when multiple threads are producing them. You might not mind having to do fiddly lock-administration when you‚Äôre setting up the <em>real</em> data flow of your program, but it definitely slows down investigation if you have to do it for every throwaway print statement!<p>But another nice thing about coroutines is that they‚Äôre easy to <em>abandon</em>. If a thread is in the middle of blocking on a lock, and you suddenly discover you no longer need that thread (and, perhaps, not the one it was blocking on either, or 25 surrounding threads), then depending on your language‚Äôs threading system, it can get very painful to arrange to interrupt all those threads and terminate them. Even if you manage it at all, will all the resources they were holding get cleaned up? (File descriptors closed, memory freed, etc.)<p>I don‚Äôt think thread systems are generally great at this. But coroutines <em>are</em>: a suspended coroutine normally has an identity as some kind of actual object in your programming language, and if you destroy or delete or free it, then you can normally arrange for it to be cleaned up in a precise manner, freeing any resources.<p>Using <code>spigot</code> as an example again: most of its coroutines run indefinitely, and are prepared to yield an infinite stream of data if you want them to. At some point the ultimate consumer of the whole computation decides it‚Äôs got enough output, and destroys the entire web of connected coroutines. Using C++ with preprocessor-based coroutines, this is reliably achieved with zero memory leaks. I‚Äôd hate to have to figure out how to terminate 250 actual system threads as cleanly.<p>Also, having a language object identifying the coroutine instance can be used for other stunt purposes. I‚Äôll talk more about that in <a href=https://shankun.github.io/posts/philosophy-of-coroutines/#named-object>a later section</a>.<p>So <em>I</em> still prefer coroutines to threads. But I have to acknowledge that my reasons are a little bit more borderline than the reasons in the previous section.<p>(However, this discussion is all about <em>pre-emptive</em> threads. <em>Cooperative</em> threads which yield to each other explicitly are a different matter, and I‚Äôll talk about those <a href=https://shankun.github.io/posts/philosophy-of-coroutines/#cothreads>later</a> as well.)<h3 id=subjective>The subjective view: why do <em>I</em> like them so much?<a aria-label="Anchor link for: subjective" class=zola-anchor href=#subjective style=visibility:hidden>#</a></h3><p>This is already the kind of essay where I acknowledge my own bias. So I should also admit the possibility that it‚Äôs not so much that coroutines are objectively amazing, but rather, that they appealed especially strongly to <em>me personally</em> for some reason that doesn‚Äôt apply to everybody.<p>I have a couple of theories about what that reason might be, if there is one.<h4 id=student-ready>‚ÄúTeach the student when the student is ready‚Äù<a aria-label="Anchor link for: student-ready" class=zola-anchor href=#student-ready style=visibility:hidden>#</a></h4><p>One possibility is that the concept was presented to me at exactly the right stage in my education.<p>I‚Äôd already been programming for a number of years when I read TAOCP, so I‚Äôd already felt for myself the frustration of having to write a piece of code ‚Äòinside out‚Äô ‚Äì that is, in the form of an explicit state machine that I‚Äôd rather have avoided ‚Äì and I hadn‚Äôt thought of any way to make that less annoying. So when Donald Knuth helpfully presented me with one, I pounced on it with gratitude.<p>But if I‚Äôd encountered the idea of coroutines earlier, <em>before</em> I‚Äôd felt that frustration, then I might not have appreciated it as much. It would have been one among many concepts that went through my brain as I read the books, and I‚Äôd have thought, ‚Äòok, fine, I expect that‚Äôs useful for <em>something</em>‚Äô, and then I‚Äôd have forgotten it (along with half of the rest of TAOCP, because there‚Äôs a <em>lot</em> of stuff in there!). And perhaps I might not have managed to recall it later, when the problem did appear in my life.<p>Conversely, the idea of coroutines might <em>also</em> have had less impact on me if I‚Äôd learned about it <em>later</em>. I have a strong suspicion that if I‚Äôd spent another (say) five years writing pieces of code inside out because I had no idea there was another option, I‚Äôd have got much <em>better</em> at writing code inside out, and much more used to it, and then the opportunity to avoid having to do it wouldn‚Äôt have seemed like such a huge boon any more.<p>Also, by that time, I‚Äôd probably have thought of some actual advantages to the inside-out state-machine style of coding, and started making use of them. There are often silver linings to be found in situations like this. In this case I can‚Äôt say for sure what they might be (since I never <em>did</em> become that practised at the skill); but one possibility that springs to mind is that an explicit list of states might make exhaustive testing more convenient (you can ensure you‚Äôve tested every state and every transition). But whatever the compensating advantage might be, once I‚Äôd noticed it, I‚Äôd have been reluctant to lose it for the sake of (as I would have seen it) not <em>that</em> much extra convenience.<p>(This is probably related to Paul Graham‚Äôs idea of the <a rel="nofollow noreferrer" href=http://www.paulgraham.com/avg.html>Blub Paradox</a>, in which you look at a less powerful language than your favourite one and you <em>know</em> it‚Äôs less powerful, because it lacks some feature you‚Äôre used to, and you can‚Äôt imagine how you‚Äôd get anything done without that. But you look at a more powerful language that has features yours doesn‚Äôt, and whatever they are, you seem to have been getting along fine without them ‚Äì so you conclude that they can‚Äôt be <em>that</em> important, and that the ‚Äòmore powerful‚Äô language isn‚Äôt that much better really.)<p>So perhaps it‚Äôs just that coroutines hit me at precisely my moment of peak frustration with exactly the problem they solve, and that‚Äôs why I adopted them with so much enthusiasm.<h4 id=clarity>They suit my particular idea of code clarity<a aria-label="Anchor link for: clarity" class=zola-anchor href=#clarity style=visibility:hidden>#</a></h4><p>When you look at an existing piece of code, there are two different ways you might want to understand it.<p>One way is: <em>on its own terms, what does this code do, and how do I modify it to do it differently?</em> For example, in the <a href=https://shankun.github.io/posts/philosophy-of-coroutines/#vs-state-machines>decompressor example</a> a few sections ago, suppose you wanted to look at the code and figure out what the compressed data format was ‚Äì either to write a specification, or to check it against an existing specification. Or suppose someone had just <em>changed</em> the specification for the compressed data format, and you had to adjust the code for the new spec.<p>For that purpose, it doesn‚Äôt really matter how the code fits into the rest of the program. In fact you don‚Äôt even need to <em>see</em> the rest of the program. You could figure out the format from <em>either</em> of the code snippets I showed (the one with a state machine, or the one that calls <code>stream.getbyte()</code>), and it wouldn‚Äôt matter at all that the rest of the program hasn‚Äôt even been written. In both cases, you can see that the code is getting a stream of bytes from <em>somewhere</em>, be it a caller or a subroutine; you can follow the logic through and see what bytes cause what effects; if you‚Äôre updating the code, you just have to follow the existing idioms for input and output.<p>But although you <em>can</em> do that level of work on either version of the code, it‚Äôs much easier to do it on the version that calls a ‚Äòget byte‚Äô function. At least, I think so; if anyone actually finds the state machine version <em>easier</em> to work with in that kind of way, I‚Äôd be interested to know why!<p>But the other way you might want to understand a piece of code is: <em>how does this fit into the rest of the program, and how can I change that?</em> If you had no interest at all in the compressed data format and no reason to think this decompressor was delivering incorrect output, but you were concerned with what was being done <em>next</em> with the data being decompressed, then you‚Äôd want to find out not ‚Äòwhat bytes are output, and how can I change that?‚Äô, but ‚Äòwhen a byte is output, where does it go next, and is it the right place, and how do I reroute it to somewhere else?‚Äô.<p>For <em>this</em> purpose, coroutines often make things less clear, rather than more. That‚Äôs especially true if they‚Äôre based on my C preprocessor system, because a person coming fresh to a function in that style is going to see some weirdo <code>crReturn</code> macro, look up the definition, be further baffled (it‚Äôs fairly confusing if you don‚Äôt already recognise the trick), and probably conclude that I‚Äôm throwing obstacles in their path on purpose! But other less ad-hoc coroutine systems have the same property; for example, in C++20 coroutines, the mechanism for dealing with yielded data values lives in a ‚Äòpromise class‚Äô type, which is inferred from the type signature of the coroutine itself by a system of template specialisations that could be almost anywhere in the source file or any of its header files. If there isn‚Äôt clear documentation somewhere in your code base, it might be legitimately quite hard to find out what happens to data after it leaves a C++20 coroutine; you might very well find that the simplest way to answer the question was to step through the yield in a debugger and see what happened next.<p>Which of these kinds of clarity is more important? Of course, both of them <em>are</em> important, because both are things you might need to understand, or fix, or update in your code. But if you spend 90% of time working on the code in one of those ways, and 10% of your time in the other way, you‚Äôre going to prefer the kind of clarity that helps you 90% of the time over the kind that helps you 10% of the time.<p>And it so happens that my own preference skews towards the first kind. Perhaps that has to do with the types of program I work on. All my life I‚Äôve gravitated to programs that solve complicated problems, because that‚Äôs the kind of thing I enjoy doing. In those programs, the individual subproblems are complicated and so are the pieces of code that deal with them; the data flow between them is a small part of the overall system. So I don‚Äôt want to make debugging each sub-piece of code harder, just for the sake of making the data flow more explicit. I‚Äôd rather each piece made sense on its own terms at the expense of the data flow being a little more opaque.<p>This is also one example of a more general question. Is it better for each program to effectively define its own local customised variant of the language it‚Äôs written in (via very elaborate support libraries, metaprogramming tricks like macros, and the like), so that the learning curve takes a long time to climb, but you can work really effectively once you‚Äôve climbed it? Or is it better to avoid diverging too much from the standard ways to use a language, so that a new programmer can get up to speed quickly?<p>Again, my preference skews toward the former. I value the ability to make my own life easier if I‚Äôm spending a long time in a code base. And I don‚Äôt much mind climbing the learning curves for other people‚Äôs differently customised coding environments, because I often learn something in the process (and maybe steal any good ideas I find). But I know that there are other people who are much more in favour of sticking to the orthodoxy.<p>(However, that‚Äôs only an argument against coroutines as long as they <em>aren‚Äôt</em> the orthodoxy. With more mainstream adoption, perhaps this argument is weakening. Generators in Python are <em>definitely</em> mainstream now: I‚Äôve occasionally had a code reviewer tell me to use them <em>more!</em>)<h2 id=technique>Techniques for getting the most out of coroutines<a aria-label="Anchor link for: technique" class=zola-anchor href=#technique style=visibility:hidden>#</a></h2><p>Of course, another reason why one person might find a language feature more useful than someone else is that they‚Äôre better at using it: better at spotting cases where it‚Äôs useful, better at getting the most out of it when they do use it. And let‚Äôs not forget the skill of <em>avoiding</em> using it when it <em>isn‚Äôt</em> useful, so that you don‚Äôt get annoyed by situations where it‚Äôs not really a good fit.<p>Here‚Äôs a collection of my thoughts on when and how to use coroutines for best effect.<h3 id=when>When to use coroutines, and when not to<a aria-label="Anchor link for: when" class=zola-anchor href=#when style=visibility:hidden>#</a></h3><p>My first general piece of advice is: <em>don‚Äôt be all-or-nothing about it</em>. Coroutines are useful in many situations, but they‚Äôre not the best tool for <em>every</em> job.<p>Programming language features are conveniences, not moral imperatives. They exist to make your life easier. So in a case where a feature <em>doesn‚Äôt</em> make your life easier, it‚Äôs OK not to use it.<p>(As far as I‚Äôm concerned, this applies to any language feature, not just coroutines. Another good example is recursion: it‚Äôs a very powerful and useful technique, but I have no time for languages that force you to use it for <em>everything</em>, even simple looping constructions. Especially if they also require you to learn all the tricks of passing extra parameters to permit tail recursion: to my way of thinking, that‚Äôs a clear sign that you‚Äôre trying to turn something into recursion that doesn‚Äôt really <em>want</em> to be written that way. Similarly, not everything needs to be a class ‚Äì sorry, Java ‚Äì and not everything needs to be immutable ‚Äì sorry, Haskell. Multi-paradigm is the One True Way, because it‚Äôs the only way that doesn‚Äôt insist that there‚Äôs a One True Way!)<p>As I said in an earlier section, precisely one of the good things about coroutines is that they give you the freedom to structure each part of a program in the way that‚Äôs most useful, independently of the rest. In particular, that includes the freedom to <em>not</em> use a coroutine in a particular situation, if you don‚Äôt want to!<p>Coroutines are useful when successive calls to a function need to behave differently, in a way that ‚Äòlooks like control flow‚Äô. That‚Äôs hard to define, but after a while, you know the kind of thing when you see it: if the full sequence of function calls wants to exhibit a number of different behaviours one after another, or repeat an action lots of times with different actions coming before and after it, or diverge into two totally different sequences of operations based on some input condition (especially if you then re-converge into some later behaviour that looks the same regardless), or most especially repeat a sequence of things that <em>also</em> include sub-repetitions and conditionals of their own. Those are all the kinds of thing that would be very natural to write using sequential code, or loops, or if-statements, or nestings of those things ‚Äì so once you recognise that ‚Äòcontrol flow nature‚Äô, consider writing it as <em>actual</em> control flow, in a coroutine.<p>On the other hand, some things just <em>don‚Äôt need</em> to be coroutines. If a function needs to do basically the same thing every time, you <em>could</em> write it as a coroutine anyway, containing nothing but an infinite ‚Äòwhile true: do thing‚Äô loop, but there wouldn‚Äôt be any real point. That‚Äôs just boilerplate that makes simple code more complicated for the sake of it. In that situation, don‚Äôt bother!<h3 id=use-cases>Types of code that might usefully become coroutines<a aria-label="Anchor link for: use-cases" class=zola-anchor href=#use-cases style=visibility:hidden>#</a></h3><p>Just waving my hands and saying ‚Äòspot things that look like control flow‚Äô isn‚Äôt the most helpful advice in the world, of course! So in this next section I‚Äôll mention some things I‚Äôve found them useful for in the past.<p>In coming up with the list, I found it convenient to categorise coroutines according to whether they yield to get inputs or to produce output or both, and to what other pieces of code.<h4 id=generator>Output only: generators<a aria-label="Anchor link for: generator" class=zola-anchor href=#generator style=visibility:hidden>#</a></h4><p>One of the simplest kinds of coroutine, and perhaps the most familiar to at least Python programmers,<sup id=footnote-python-coroutines>2</sup> is the kind that yields values to a consumer, but its input comes either from the parameters it was passed on creation, or from ordinary function calls.<p>(Of course, those function calls might be to the ‚Äònext‚Äô method of other generators, in which case you can look at this as taking input from a different coroutine after all! But only semantically: in a <a href=https://shankun.github.io/posts/philosophy-of-coroutines/#resumable-callee>later section</a> I‚Äôll mention a performance concern relating to this.)<p>If a coroutine only generates output, then there‚Äôs a third alternative to making it a coroutine (as well as the state-machine and thread options discussed earlier): you can simply make it compute <em>all</em> its output ahead of time, and return it in full as a list or array. This gives you all the same advantages in terms of writing the control flow in a natural and readable style. For example, here are two versions of a simple function that finds the ‚Äòrecord-breaking‚Äô values in a list, i.e. every value that is larger than all the ones before it:<pre class=z-code><code><span class="z-text z-plain">\# Written as a generator
def record_breaking_values(inputs):
    it = iter(inputs)
    best = next(it)
    yield best
    for value in it:
        if value > best:
            best = value
            yield best
</span></code></pre><pre class=z-code><code><span class="z-text z-plain">\# Ordinary function that just returns a list
def record_breaking_values(inputs):
    it = iter(inputs)
    best = next(it)
    records = [best]
    for value in it:
        if value > best:
            best = value
            records.append(best)
    return records
</span></code></pre><p>In that example, the ‚Äòjust return a list‚Äô function is one line longer because of the actual <code>return</code> statement. But that‚Äôs a trivial concern. (And I could have made it a line shorter instead, if I‚Äôd eliminated the <code>best</code> variable and just checked <code>records[-1]</code>, but then the two pieces of code wouldn‚Äôt have matched so clearly.) The point is that the control flow inside the function is exactly the same, and the algorithm is equally clear to a reader, and equally easy to modify.<p>The advantages of the coroutine version over this are space and time. The space advantage is obvious: if that returned list is <em>long</em>, then it‚Äôs nicer to avoid ever having to store all of it in memory at once, and instead, throw away each value after it‚Äôs been used. This doesn‚Äôt reduce flexibility, because (at least in Python) there‚Äôs a very concise idiom for turning the output of a generator back into a list at the call site if you really do need it all available at once.<p>(Of course, this is exactly the same reason that Unix provides a system for piping data between processes, in place of the more obvious approach of writing the data to an intermediate temporary file. One way to look at coroutines is that they‚Äôre the in-process analogue of a Unix pipe!<sup id=footnote-knuth>3</sup>)<p>The time advantages are more subtle. In fact it‚Äôs not immediately obvious how there‚Äôs a time advantage at all: surely the same <em>calculations</em> have to happen, whether we select the record-breaking values in advance of using them or interleaved with using them.<p>And, of course, that‚Äôs true. If we really are going to use <em>all</em> the returned data, then the <em>overall</em> time taken to do the calculations is the same.<p>But sometimes we‚Äôre concerned with intermediate timings as well as the overall time. For example, if an interactive program needs to output the next value in a sequence when the user performs a UI action, then it‚Äôs nicer if the time taken to compute that value is reasonably bounded, rather than the program <em>occasionally</em> saying ‚ÄúHold on, I need to precompute the next 10000 items before I can give you the first of them‚Ä¶‚Äù<p>And sometimes we‚Äôre <em>not</em> intending to use all of the returned data. Another way to look at generator-style coroutines like this is that they‚Äôre a form of <em>lazy evaluation</em>: no value in the logical output list is computed until another part of the program actually needs it, and that means that if <em>nothing</em> ever needs the value, we never wasted the time of computing it. In the interactive scenario, you‚Äôd be <em>particularly</em> annoyed at the program wasting your time on precomputing the next 10000 output values if you were intending to look at the first ten and then quit the program!<p>One special case of this, where you <em>really can‚Äôt</em> compute the entire intermediate list up front, is if the intermediate list is potentially infinite! Some natural generators will just keep on and on generating data, and the only way to <em>ever</em> stop them is by their consumer getting bored and discarding them.<p>In any case, the takeaway from this is: any time you see a function that constructs and returns a list, if it functions in more or less ‚Äòappend only‚Äô mode (that is, it builds the list up from start to finish, and doesn‚Äôt have to refer back to an unlimited number of the existing items to decide what to append next), it‚Äôs worth considering whether it would be clearer written as a generator-style coroutine instead, if your language makes it convenient.<h4 id=consumer>Input only: consumers<a aria-label="Anchor link for: consumer" class=zola-anchor href=#consumer style=visibility:hidden>#</a></h4><p>If I‚Äôve had a section for output-only coroutines, then obviously I need a section for input-only ones.<p>At least in normal languages where a coroutine is a special kind of function, these don‚Äôt often seem to be very useful. It‚Äôs commonplace to have one piece of code producing data and passing it to another that consumes it, but you usually only need <em>one</em> of them to be a special coroutine- or generator-type function; the other can just be a normal non-coroutine-style function that invokes the producer coroutine when it needs another item, e.g. by calling <code>next()</code> on a Python generator.<p>You generally <em>can</em> write the two pieces of code the other way round, so that the producer is an ordinary function and the consumer is a coroutine. But it doesn‚Äôt often seem to be the easiest way to do things.<p>Partly, that‚Äôs because languages don‚Äôt make it quite as easy. In Python, for example, you <em>can</em> pass data to a generator by calling <code>g.send(value)</code> and retrieve it inside the routine as the return value of the <code>yield</code> operator; but it‚Äôs awkward that the generator starts off in a state where its code hasn‚Äôt run at all yet, so you have to faff with it to get it to the point of first input. But other systems don‚Äôt have this awkwardness, or at least, you can arrange not to have it; for example, C++20 coroutines allow you to configure whether your coroutine should start off suspended, or run to the point of first yield / await. (Or other options.)<p>More importantly, the consumer end of the data pipeline is probably the one that‚Äôs taking the program‚Äôs main action. So you probably want to make <em>sure</em> it completes. I mentioned above that one of the virtues of coroutines is often that they‚Äôre safely discardable (via the language‚Äôs cleanup or destructor system) if you want to abandon the computation half way through ‚Äì but that stops being a virtue if the computation in question is the one you want to be sure of <em>not</em> accidentally abandoning!<p>So I think it generally makes sense to have the producer end of the system be an easily discardable coroutine, and the consumer end be ‚Äònormal‚Äô code. That way, if the producer encounters an exceptional condition and can‚Äôt carry on producing data (e.g. a file it was reading from ended abruptly, or contained invalid data), the consumer can catch the exception and ensure it does something sensible, like switching to a fallback strategy, or deleting its half-written output file, or at the very least producing a useful error message. You probably don‚Äôt want it to be silently discarded.<h4 id=adapter>Separate input and output: adapters<a aria-label="Anchor link for: adapter" class=zola-anchor href=#adapter style=visibility:hidden>#</a></h4><p>Once you‚Äôve connected a producer to a consumer, the next natural step is to have something in the middle be both at once, consuming values from one side and emitting them to the other side.<p>The <code>record_breaking_values()</code> example I showed in Python earlier is already an example of one of those, depending on how you look at it. If you pass it a static thing like a list, then it‚Äôs just a producer coroutine, doing computation based on its original parameters; but if you pass it another generator as an argument, then it ‚Äòyields‚Äô to that generator by calling <code>next()</code> on it, and yields to its consumer by using the actual <code>yield</code> statement.<p>There are lots of natural adapters of this kind that you might usefully write as coroutines. Again, it‚Äôs not <em>always</em> the right thing to write them that way, but it can very easily be.<p>For example, here‚Äôs a tiny Python generator function that I put in a lot of my programs, because I often find I want to use it, and despite being so simple, it‚Äôs not in the standard library:<pre class=z-code><code><span class="z-text z-plain">def cyclic_pairs(inputs):
    it = iter(inputs)
    prev = first = next(it)
    for curr in it:
        yield prev, curr
        prev = curr
    yield prev, first
</span></code></pre><p>Given a list of inputs such as <em>a</em>,<em>b</em>,<em>c</em>,<em>d</em>,<em>e</em>, this returns each pair of adjacent list elements as a tuple, so that you get (<em>a</em>,<em>b</em>), (<em>b</em>,<em>c</em>), (<em>c</em>,<em>d</em>) and (<em>d</em>,<em>e</em>) as outputs.<sup id=footnote-pairwise>4</sup> Finally, it produces the pair that ‚Äòwraps round the end‚Äô, consisting of the last element and the first, in this case (<em>e</em>,<em>a</em>).<p>If you knew the input was in the form of an already-complete list, you could write this easily enough by iterating over the list indices, and returning <code>(list[i], list[(i+1) % len(list)])</code>. But if you write it in this form then it can take any iterable as input ‚Äì including another generator, which perhaps doesn‚Äôt even know what all its output is going to be yet.<p>What makes this a natural use case for generators? It‚Äôs the special cases at the start and the end.<sup id=footnote-StopIteration>5</sup> If every input value was treated exactly the same, then you could just as easily write this as a tiny class storing the <code>prev</code> variable, with a <code>__call__</code> method that constructed each tuple and updated <code>prev</code>. But in reality, the <em>first</em> element from the input iterator must be treated specially (in that we still put it into <code>prev</code> but don‚Äôt yield a tuple at all), and when we reach the end, we have an extra thing to do (namely yielding that final wraparound tuple). You <em>could</em> fake that up in your class, perhaps by special-casing <code>prev=None</code> (but what if the input iterator contained some legitimate <code>None</code> values itself?), or more likely by a system of flag variables in the class. But then you‚Äôre in ‚Äòexplicit state machine when you didn‚Äôt really want one‚Äô territory ‚Äì so it‚Äôs easier just to write code like this, which says directly, in sequence, what things you want to do, in what order.<p>Adapters like this fit nicely into the ‚ÄòUnix pipeline‚Äô model, as I described a couple of sections earlier. So they‚Äôre still only a performance improvement, in space and maybe time, over doing things the pedestrian way with intermediate lists.<h4 id=protocol>Input and output talking to the same entity: protocols<a aria-label="Anchor link for: protocol" class=zola-anchor href=#protocol style=visibility:hidden>#</a></h4><p>Now we come to the case where coroutines deliver more than a performance optimisation: where they do both input and output, but the output goes to the <em>same entity</em> that‚Äôs providing the input.<p>It doesn‚Äôt exactly matter what that entity <em>is</em>: it might be another part of the same program, or it might be a human interacting with the program, or it might be another program at the far end of a network connection. But if the entity at the other end has to see your outputs so far before deciding what your next input is, and you have to do the same to generate <em>your</em> next output, then this type of coroutine <em>can‚Äôt</em> be rewritten as a function that just generates the whole output list up front.<p>One of Knuth‚Äôs examples in TAOCP was a piece of code that implements a chess-playing algorithm. That‚Äôs exactly a case of this kind: each time you output a move, you have to wait to find out what my responding move is before you can even decide what moves you can <em>legally</em> play next, let alone which one is best. You can imagine a function like this talking to a human player through some user interface code ‚Äì but you could also set two copies of it playing against each other, yielding control flow back and forth each time one of them made a move.<p>(On the other hand, I don‚Äôt know that chess is a good case for coroutine style, because as far as I know, chess algorithms these days work by analysing whatever board position they‚Äôre given, and if they retain any state from the previous move it will only be in the form of cached evaluations of potential future game states. So a ‚Äònext chess move‚Äô function probably wouldn‚Äôt want to be a coroutine after all. But some games‚Äô rules ‚Äì or optimal strategies ‚Äì <em>do</em> go through sequential phases that would suggest a coroutine-structured playing function.)<p>But gaming is a frivolous use of this idea. A much more practical one is network protocols. The reason coroutines occur throughout PuTTY is because they‚Äôre so useful for implementing sequential network protocols, under the constraint that the top level of your program is an event loop waiting for whatever input event happens next out of network connections, user input, and sometimes timers.<p>A good example is the key exchange layer of the SSH protocol. The two sides exchange setup packets that list all the different cryptographic algorithms they‚Äôre prepared to use; out of those options, they select the best one that both sides understand, and then exchange several packets to perform a key exchange according to that algorithm. Having done that, each side installs a new set of cryptographic keys, and then exchange encrypted packets implementing the actual interactive login session ‚Äì until one side decides it‚Äôs time to refresh the cryptography, and sends another key-exchange setup packet.<p>The sequence of packets within each key exchange method is different; only one of those sequences is used; and the whole thing takes place in a loop, restarting every time a new key exchange is needed. These are all markers of ‚Äòlooks like control flow‚Äô, and make it very natural to write the code in a style like this:<pre class=z-code><code><span class="z-text z-plain">function key_exchange() {
    send(KEXINIT); // key exchange initialisation packet
    wait for other side‚Äôs KEXINIT;
    outer loop {
        decide what key exchange type we‚Äôre doing;
        if (it‚Äôs this type) {
            send this;
            expect that;
            send some response;
        } else if (it‚Äôs some other type) {
            expect the other side to send a packet first;
            send a totally different response packet;
        } else {
            // potentially more and more cases like this
        }
        install new keys;
        inner loop {
            get next packet from server;
            if (it‚Äôs KEXINIT) {
                // time to do another key exchange
                send our own KEXINIT;
                break from inner loop; // so we return to top of outer loop
            } else {
                pass the packet on to the next layer up;
            }
        }
    }
}
</span></code></pre><p>This function is structured as if it‚Äôs calling a subroutine to retrieve the next packet from the other side of the connection. But in reality, it‚Äôs going to <em>be</em> called each time a packet arrives ‚Äì so we have to either make it a coroutine, or turn it inside out by setting up an explicit state machine. I think keeping the code in this nice sequential form is <em>far</em> preferable, so coroutines it is!<p>In fact, this kind of pattern recurs throughout SSH. The ‚Äòpackets‚Äô that are arriving in the code shown above are built up from the actual stream of bytes coming from the network by means of reading an initial few bytes giving the packet length, then reading that many more bytes to get the rest of the packet, then reading the authentication code at the end (if any); doing all the decryption and integrity checking, and finally, having a packet available to output. This code too wants to be written in a style where it calls a ‚Äòwait for <em>n</em> more bytes of network data‚Äô function ‚Äì but, again, it‚Äôs <em>being</em> called from the main event loop when data comes in on the network socket. So that layer too is a coroutine, in the simpler ‚Äòadapter‚Äô style where the input and output are separate: it consumes bytes from the event loop, and periodically yields a completed packet to the key-exchange layer shown above.<p>And the ‚Äònext layer up‚Äô ‚Äì mentioned at the end of the above pseudocode ‚Äì does all of this <em>again</em>. Initially, it‚Äôs the user-authentication layer of SSH, which might prompt for a password, or use a public key, or do something more complicated still; any of those might fail, in which case it has to loop back round and try the next fallback approach. So that‚Äôs very natural to write as a coroutine as well. The whole structure of SSH is almost <em>designed</em> to be three coroutines stacked on top of each other!<p>(Once you‚Äôve actually logged in, things get a bit less coroutiney. The ‚Äòconnection layer‚Äô that takes care of running shell sessions is much less sequential. In PuTTY, it‚Äôs still structured as a coroutine, but <em>mostly</em> that coroutine sits in an endless while loop just responding to each packet as it comes in. That would normally be the kind of boring stateless code that it‚Äôs not really worth making into a coroutine at all ‚Äì except that right at the start we have to do a few setup tasks like opening an initial session channel, so the coroutine consists of a small amount of sequential setup code and <em>then</em> the boring while loop.)<p>This isn‚Äôt an article about SSH alone, of course. That‚Äôs just a particularly good example. Many other network protocols are just as sequential. For example, PuTTY‚Äôs code to connect via an HTTP proxy is also structured as a coroutine.<p>In fact, that code has an extra wrinkle too. Sometimes, when authentication fails in HTTP, you can try again within the same network connection; but sometimes you have to close the connection to the HTTP server and open a fresh one. So there‚Äôs not just a sequence of operations <em>within</em> a network connection: there‚Äôs also a sequence of <em>connections</em> involved. In PuTTY, this is all handled with a single HTTP proxy coroutine: sometimes it yields a special return value to its caller saying ‚Äòplease close the connection, open a fresh one, and call me back when that one‚Äôs ready‚Äô. Then it can go back round to the top of its loop and continue trying to authenticate.<h4 id=uc-general>More general, and miscellaneous<a aria-label="Anchor link for: uc-general" class=zola-anchor href=#uc-general style=visibility:hidden>#</a></h4><p>In all of the previous subsections, I‚Äôve shown coroutines with at most one input channel and at most one output. Of course, you can also have more than one of each.<p>There are two conceptually different ways that a coroutine might consume two input streams. One is that it yields to a <em>specific</em> input stream each time it wants a new value, so that it‚Äôs in control of how fast the two input streams deliver data. There are lots of examples of this in purely computational contexts: consider, for example, a function that merges two sorted lists, or zips together two or more streams by making a tuple of one element from each stream, or other things of the kind you might find in Python <code>itertools</code>.<p>The other approach, more suited to coroutines that are doing asynchronous I/O, is that the coroutine might yield in such a way that it will be resumed when <em>any</em> of the input streams has data available ‚Äì no matter which one it is. For example, some of PuTTY‚Äôs SSH coroutines have to handle both network data and user input: the user authentication coroutine will sometime respond to a network packet by presenting a password prompt to the user, and then it has to receive the password that the user typed in return and construct the network reply packet. But there‚Äôs no guarantee of which is going to happen next, out of a network packet arriving and the user pressing a key. So the coroutine is resumed whenever <em>either</em> one happens.<p>That style of coroutine is tricky to write well, and I have some advice about it in later sections. Here, I just mention that it‚Äôs among the possibilities.<p>Another slightly unexpected ‚Äòstunt‚Äô use of coroutines is to use them to reconstruct the call-stack structure of ordinary subroutines: replace every normal function call with a coroutine-style yield operation which makes a newly constructed instance of the subroutine start running next, and whenever a coroutine terminates, resume the one that invoked it.<p>Why on earth is <em>that</em> a useful thing to do, you might ask? If we want something that behaves like an ordinary function call stack, doesn‚Äôt the typical programming language have one built in already?<p>I‚Äôve found that useful in the past as a means of working around limitations of the true function call stack. In Python, for example, you can only nest function calls about 1000 deep, and I‚Äôve sometimes needed to recurse much more deeply than that ‚Äì e.g. running graph algorithms over a DAG with very long chains, such as a git repository. It‚Äôs surprisingly convenient to rewrite function calls and returns in a style like this‚Ä¶<pre class=z-code><code><span class="z-text z-plain">\# Before
def routine(x):
    i = subroutine(x+1)
    j = subroutine(x*i)
    return i+j
</span></code></pre><pre class=z-code><code><span class="z-text z-plain">\# After
def routine(x):
    i = yield Call(subroutine(x+1))
    j = yield Call(subroutine(x*i))
    yield Return(i+j)
</span></code></pre><p>‚Ä¶ and then have a small piece of ‚Äòexecutor‚Äô code which maintains a stack of currently active generators in an array, and contains a loop which always resumes the innermost generator on the stack. If that yields a <code>Call</code> object containing another generator, push that one on the stack, so that it will be the one resumed next; if it yields a <code>Return</code> containing a return value, pop the stack, and pass the value to the new topmost generator when it‚Äôs resumed.<p>When I‚Äôve done this in the past, my reason has always been to get round Python‚Äôs recursion depth limit. But I can think of other reasons for reifying your stack into explicit data objects.<p>One reason is so that those objects can be saved and restored. Given the right language support, you might be able to tweak a system like this to allow the entire state of a half-finished recursive computation to be written out to a disk file; then the program could be terminated, and when run again later, reload the file and carry on from where it left off. (Although you would also need a method of saving the <em>internal</em> state of each coroutine, which might be harder.)<p>Another is that once you replace the language‚Äôs built-in control flow with your own imitation, it becomes easy to extend or modify it. You could imagine adding a third kind of data object alongside <code>Call</code> and <code>Return</code> which had a different effect on the executor. For example, you might have two or more call stacks and switch between them, implementing ‚Äòco-threads‚Äô (which I‚Äôll talk about more in <a href=https://shankun.github.io/posts/philosophy-of-coroutines/#cothreads>a later section</a>).<p>In fact, perhaps you might decide that keeping the active function calls in an array was itself a limitation you wanted to go beyond. Not all languages insist on a linear stack in that way. Scheme, for example, has multiple control-flow features (continuations and <code>dynamic-wind</code>) with the effect that the stack is replaced by a general DAG of function activations, and several branches of the DAG can be resumable. (This also means that the stack frames must be subject to garbage collection.) A technique like this could be used to implement continuations in languages that don‚Äôt have them natively!<p>Speaking of messing about with control flow, another stunt use of coroutines ‚Äì in the right context ‚Äì is to invent your own control flow <em>statements</em>, alongside standard things like ‚Äòfor‚Äô, ‚Äòwhile‚Äô and ‚Äòif‚Äô.<p>Some languages encourage you to invent control flow primitives of your own, by means of making it easy to pass blocks of code to a function call:<pre class=z-code><code><span class="z-text z-plain">function my_control_structure(code) {
    set up stuff;
    while (some condition)
        code.run();
    clean up stuff;
}

# now the braced block becomes the ‚Äòcode‚Äô parameter to the above function
my_control_structure {
    statements;
}
</span></code></pre><p>The <em>wrong</em> way to implement this language feature is to literally wrap up the code block at the call site into a lambda function, and have the control-structure function execute the block using a normal function call. The problem with this is that then the code blocks don‚Äôt really run in the context of the containing function. They might be able to access the function‚Äôs local variables, by virtue of lexically scoped lambda capture, but the control flow is isolated. If one of those blocks executes a ‚Äòreturn‚Äô statement, then it only returns from the <em>block</em>, not the containing function (as it would if you executed the same return inside, say, a while loop). And if you want to ‚Äòbreak‚Äô from a loop implemented by these means, then executing the ‚Äòbreak‚Äô statement inside the lambda doesn‚Äôt work at all. This all makes your artificial control structure look ‚Äòsecond class‚Äô compared to the language‚Äôs built-in ones.<p>Instead of doing that, the <em>right</em> way ‚Äì and some languages actually do this, such as Ruby ‚Äì is for the control-structure function to be a coroutine. Instead of its code blocks being ordinary functions that it calls in the ordinary way, they should remain in the context of the original function, and the control structure should <em>yield</em> back to the original function, telling it which block to run next. Then custom control structures would be just as good as the standard ones, and wouldn‚Äôt behave surprisingly differently in edge cases.<sup id=footnote-if-for-while>6</sup><p>Of course, if you‚Äôre not using a language that already works this way, then finding a way to implement it can be tricky or impossible. But once I just about found a way to do it in C, combining my <a rel="nofollow noreferrer" href=https://www.chiark.greenend.org.uk/~sgtatham/coroutines.html>preprocessor coroutine system</a> with <a rel="nofollow noreferrer" href=https://www.chiark.greenend.org.uk/~sgtatham/mp/>another preprocessor trick</a> for making custom control structures at all. In C++ you might do it more easily by having the macro at the call site expand to a range-<code>for</code> loop. It will depend on the language.<h3 id=large-and-small>Coroutines large and small<a aria-label="Anchor link for: large-and-small" class=zola-anchor href=#large-and-small style=visibility:hidden>#</a></h3><p>That ‚Äòcontrol flow nature‚Äô that makes a piece of code a natural fit for coroutines can occur at all scales, from the largest to the smallest.<p>Among the examples in previous sections, I‚Äôve already shown examples at both ends of that spectrum. Some of the largest and most complicated parts of PuTTY‚Äôs SSH code, such as the key exchange or the user authentication protocol layers, are implemented as a giant coroutine. On the other hand, that tiny <code>cyclic_pairs</code> Python generator is not far off being the smallest possible piece of coroutine-shaped code that‚Äôs worth bothering to write.<p>It‚Äôs also worth considering various ways to slice up, or break down, the problem. In SSH the multiple protocol layers lend themselves nicely to being a coroutine <em>each</em>; they‚Äôd be much more unwieldy implemented as one single one. On the other hand, for some network protocols, a single coroutine might be suitable for running the entire network connection.<p>And in some other situations, you might have a variable number of coroutines, with each one tracking the progress of a single transaction <em>within</em> a network protocol. I have an example of that too in the PuTTY code: the SSH agent, Pageant, is mostly a non-coroutine loop that answers queries in whatever order they arrive ‚Äì but whenever a client of the agent requests a signature from one of Pageant‚Äôs private keys, it spawns a small coroutine to handle the progress of <em>that specific signing request</em>, because sometimes it will need to prompt the user for a passphrase, loop round again if that didn‚Äôt work, be prepared to abandon the attempt if the user clicks Cancel, etc. This is all control-flow-shaped code ‚Äì but it suited the problem best to have a coroutine for <em>each</em> signature request, not one for the whole connection.<p>So, when you‚Äôre on the lookout for coroutine-shaped parts of a program, don‚Äôt forget to look for the large ones, the small ones, and everything in between!<h4 id=activation-energy>Activation energy<a aria-label="Anchor link for: activation-energy" class=zola-anchor href=#activation-energy style=visibility:hidden>#</a></h4><p>Of course, depending on the language, that might not be good advice after all.<p>In Python, writing a generator is made extremely easy. It‚Äôs no harder than an ordinary function; you just write <code>yield</code> at least once in the body, and everything is taken care of for you. So there‚Äôs no reason I <em>shouldn‚Äôt</em> write things like that <code>cyclic_pairs()</code> function as a generator, if it‚Äôs only a tiny bit clearer. There‚Äôs essentially no cost. The next five lines of my code would look a bit nicer as a generator? Fine, dash off a generator, why not?<p>But if you were writing in C++20, there‚Äôs a much higher barrier to entry. C++20 coroutines are <a rel="nofollow noreferrer" href=https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/coroutines-c++20/>very expensive</a> to set up ‚Äì not so much in runtime cost (though I‚Äôm sure there is <em>some</em>), but in programming effort. You have to write a whole ‚Äòpromise class‚Äô with lots of methods explaining how the coroutine should behave in this or that circumstance. If the only actual coroutine you wanted to write was going to save you (say) five lines of code, then writing a whole C++20 promise class from scratch wouldn‚Äôt be a net win, because it would cost you a hundred or so lines to set up the necessary infrastructure. You have to <em>really want</em> coroutines to bother setting them up in a C++20 program.<p>However, C++20 does let you reuse the promise class for multiple coroutines that want to work in similar ways. So if you had <em>twenty</em> five-line snippets you wanted to write that way, it might become worth it. But if you‚Äôd already written the first 19 in ordinary non-coroutine style, then when you got to the break-even point, it might <em>still</em> not seem worth your while to go to the effort of writing the promise class <em>and</em> converting all the existing cases to use it‚Ä¶<p>(Also, this is only a temporary complaint about C++. C++23 will bring <code>std::generator</code>, which should make this kind of simple job <em>actually</em> simple again.)<h3 id=queues>Combine with input queues<a aria-label="Anchor link for: queues" class=zola-anchor href=#queues style=visibility:hidden>#</a></h3><p>In TAOCP‚Äôs presentation of coroutines, and many others, the whole point is that you yield to the other piece of code precisely when you have data to transfer. If a producer is constructing a stream of objects and sending them to a consumer, then you write the producer as if it has a function it can call for ‚Äòhere, I have this object for you‚Äô, and you write the consumer as if it has a function for ‚Äòok, what‚Äôs my next object?‚Äô, and there‚Äôs exactly one pair of transfers of control for each object that traverses the boundary.<p>But sometimes that‚Äôs not the best way to do it. When it‚Äôs not, don‚Äôt do it that way!<p>One case where it‚Äôs awkward is the <a href=https://shankun.github.io/posts/philosophy-of-coroutines/#uc-general>case I mentioned previously</a> of having a coroutine that can receive more than one type of input, and doesn‚Äôt know which one will come next. For example, PuTTY‚Äôs SSH authentication layer used to implement password / passphrase prompts itself, so it received a stream of events in which SSH packets and user keystrokes were interleaved, and any call to the main userauth coroutine might be providing <em>either</em> of those kinds of event.<p>But if you‚Äôre writing code in a coroutine-structured style, then in a lot of cases, you know which input type <em>you</em> want to see next ‚Äì and it‚Äôs very annoying to have to put a handler for the other one after each yield. For example, if you‚Äôre <em>not</em> in the middle of presenting a password prompt to the user, what do you do with a keystroke when you receive one? Conversely, if you‚Äôre waiting for the user to finish typing a password, and another network packet comes in, what do you do?<p>In those early versions of PuTTY, I mostly ignored the problem. While the userauth coroutine was waiting for network packets, if it was called with a keystroke, it would simply discard it ‚Äì drop it on the floor.<p>An effect of that was that you couldn‚Äôt open a PuTTY window and immediately start typing the first shell command you wanted to run in your session, <em>even</em> if you knew that authentication was going to succeed without needing any prompts (say, because you were using an SSH agent). You‚Äôd expect to be able to ‚Äòtype ahead‚Äô, and have your keystrokes buffered until the main shell session was ready to use them. Instead, for as long as authentication hadn‚Äôt finished yet, they‚Äôd just be thrown away.<p>After a while, that became annoying. I wanted keystrokes to <em>never</em> be dropped. But I also wanted to write my userauth coroutine in the most convenient way. What to do?<p>Instead of passing SSH packets or keystrokes directly to the userauth coroutine, I made two <em>queues</em> in the SSH layer, to hold SSH packets and user input respectively. So when the lower layer had a packet to deliver to the userauth layer, it wouldn‚Äôt pass the packet directly to the userauth function; instead, it would put the packet on the userauth layer‚Äôs incoming packet queue, and then call the userauth function with no arguments, just saying ‚Äòhello, it‚Äôs worth waking up and checking your queues‚Äô. And the same went for user input.<p>This solved my problem. Now, when userauth knows the next thing it‚Äôs expecting is an SSH packet, it can run a little resume loop that says ‚ÄòIs there a packet waiting in my queue? If not, suspend, and look again when we‚Äôre resumed.‚Äô If user input arrives in the meantime, it goes on the <em>other</em> queue, and will still be there when the userauth layer gets to a different place in the code where it needs some user input ‚Äì or perhaps it won‚Äôt be consumed by userauth at all, and will be picked up by the connection layer (the one that runs shell sessions after you log in), so that you <em>can</em> type ahead into the shell prompt you expect to see later.<p>In particular, sometimes the userauth layer wouldn‚Äôt need to suspend itself at all. It would look at its queue and discover there was <em>already</em> something there, and charge straight on without ever having to yield.<p>(In C, using preprocessor coroutines, this little loop was the easiest way to wait for a particular one of the possible input events. If I were rewriting this code using the C++20 coroutine system, I could formalise it, by defining a specific <code>co_await</code> idiom for ‚Äònow I want to wait until there‚Äôs something in this queue‚Äô. That could be set up so that it would avoid resuming the coroutine <em>at all</em> until that queue stopped being empty ‚Äì and it could also automate the initial check of ‚Äòmaybe we don‚Äôt even need to suspend at all because we already have a packet waiting‚Äô.)<p>Another case where queues are useful is if the data objects are very small and numerous. For example, if you wanted to write a coroutine that received a stream of <em>bytes</em> from another part of the program ‚Äì say, because the producer was a file decompressor, or the consumer was a file compressor ‚Äì then it would be bad for performance to require two coroutine-transfer overheads per individual byte. Instead, the producer would prefer to provide as many bytes in one go as it conveniently can (especially since it might have read lots of them in a block from a file, or a decompressor might have generated a lot at a time from a run-length record or an LZ77 copy). But then the consumer might not need exactly that many for <em>its</em> next operation. So it would make sense to append bytes to a queue (probably in the form of a circular buffer, or chain of small buffers, or something else space-efficient per element). The producer could generate as much data as was convenient (or would fit in the buffer), and then yield at the point when producing the next byte would need actual effort; then the consumer could eat as much of the queue as possible, and yield back when it wanted more bytes than were available.<p>But, just like I‚Äôve been saying about everything else in this article, don‚Äôt take it to extremes. Using an input queue or buffer isn‚Äôt <em>always</em> the right thing. One of the effects of representing a stream of data as a coroutine is that the stream is lazily evaluated: no element is computed until it‚Äôs really needed. So if some or all of the data values are <em>expensive</em> to compute, then either don‚Äôt use an intermediate queue at all, or else make sure you never put more than one thing in the queue <em>unless they‚Äôre easy</em>.<p>(<code>spigot</code> is a good example of this problem. The coroutines in that are mostly generators that yield streams of integer matrices. For some generators, the matrices are uniform and simple; for others, an occasional matrix is <em>exceptionally</em> costly to compute. So in that application, queues didn‚Äôt seem worth the effort; yielding exactly once per matrix is the simplest way to be absolutely sure that we never even start trying to compute a matrix until its consumer knows it‚Äôs needed.)<h3 id=handlers>Combine with ambient pre-filters<a aria-label="Anchor link for: handlers" class=zola-anchor href=#handlers style=visibility:hidden>#</a></h3><p>Here‚Äôs one more trick on the theme of ‚Äòonly use coroutines for the coroutine-shaped parts‚Äô.<p>Sometimes, a stream of data contains things that have to be handled sequentially, <em>interleaved</em> with things that should be handled the same at any time.<p>An example of this is the key-exchange ‚Äòtransport layer‚Äô of SSH. This includes all the packet types I mentioned <a href=https://shankun.github.io/posts/philosophy-of-coroutines/#adapter>earlier</a> (the <code>KEXINIT</code> start packet, various sequences of packets to perform different key exchanges, and a <code>NEWKEYS</code> message to signal that you‚Äôre about to switch to the newly generated encryption keys); but it <em>also</em> includes a small number of messages that can arrive at any time and must be handled correctly when they do.<p>The handling isn‚Äôt always <em>difficult</em>. One key example is the <code>IGNORE</code> message, which either side may send at any time, and the other side ‚Äì as suggested by the name ‚Äì must ignore it. That‚Äôs the single easiest possible way to handle a packet.<p>But now suppose you‚Äôve written your transport layer in the form of a coroutine. There will be lots of separate points in the code where you yield, wait for the next packet, and expect it to be of a particular type (typically, whatever comes next in the key exchange you‚Äôre performing). Now <em>every one</em> of those yield points has to have an extra check for the <code>IGNORE</code> message, which loops round to get the packet after it:<pre class=z-code><code><span class="z-text z-plain">// What you‚Äôd like to write
function transport_layer() {
    // ...
    pktin = co_await PacketInputQueue;
    if (pktin.type != FOO)
        disconnect("got packet type %s, wanted FOO");
    // ...
}
</span></code></pre><pre class=z-code><code><span class="z-text z-plain">// What you have to write instead every time
function transport_layer() {
    // ...
    while (true) {
        pktin = co_await PacketInputQueue;
        if (pktin.type == IGNORE)
            continue;
    }
    if (pktin.type != FOO)
        disconnect("got packet type %s, wanted FOO");
    // ...
}
</span></code></pre><p>If you‚Äôve got even ten or twenty yield points in your coroutine, this is a lot of tedious boilerplate code ‚Äì and worse, a lot of places to accidentally leave out that extra clause, and introduce a latent bug that will only show up when a server actually <em>does</em> happen to send <code>IGNORE</code> in that particular context. And it only gets worse when you have to support the rest of SSH‚Äôs any-time message types. Ignoring that <code>IGNORE</code> message <em>isn‚Äôt</em> the easiest thing in the world, it turns out!<p>So don‚Äôt do it that way.<p>Instead, a structure I‚Äôve found useful is to have a ‚Äòpre-filter‚Äô on the queue, which isn‚Äôt part of the coroutine at all. Whenever new packets arrive on the queue, they‚Äôre seen first by the pre-filtering code. That will handle things that can arrive at any time, like <code>IGNORE</code> and its friends. The only remaining packets are the ones that <em>are</em> expected to appear in a sensible sequence that‚Äôs natural to write as a coroutine. And <em>those</em> are passed to the main transport-layer coroutine, which can handle them in the natural way, without having to do anything at a yield point that isn‚Äôt related to what that <em>particular</em> yield point is trying to achieve.<p>So this lets the coroutine handle only the coroutine-shaped parts of the protocol: the things that happen in sequences, or conditionally, or in loops, and look like control flow. Meanwhile, the awkward things that can interrupt your nice orderly control flow are simply removed from the coroutine‚Äôs view of the input and handled by a central dispatch function. Then each style of code can do what it does best; or, looking at it the other way round, each part of the problem can be solved in the way that‚Äôs most natural for that part.<p>(<em>How</em> do you wedge this pre-filter system into a world organised by coroutines? PuTTY uses C preprocessor coroutines, in which there‚Äôs a convenient slot within the coroutine function itself ‚Äì just before the <code>crBegin</code> macro ‚Äì where you can put code that will run on every single resume, before control is transferred to wherever the function last suspended. For languages which implement coroutines natively, that slot probably doesn‚Äôt exist, so you‚Äôll have to do it the more pedestrian way of hiding the coroutine resume operation behind a separate wrapper function, and having that wrapper do any pre-filtering necessary before resuming the actual coroutine.)<p>The SSH packets I‚Äôve been talking about here are fixed. They can be sent at any time at all in the protocol, and they <em>never</em> want to be handled by the coroutine code. But in other situations, there are message types which you <em>sometimes</em> want to handle in the sequential coroutine code, even though at other times you‚Äôd rather they were handled by the pre-filter so that the sequential code can concentrate on something else.<p>In this situation, you can still keep the sequential code in the coroutine and the any-time handlers in the nicely separate filter function, by means of the coroutine setting variables that affect how much the filter function filters. Then the coroutine can dynamically control what subset of the inputs it sees, and always focus its attention on the ones that behave sequentially.<p>Those variables can be as simple or as complicated as you like. It might be as simple as having a single boolean flag saying whether the coroutine would currently like to see whatever kind of thing is only sometimes interesting to it. Or you might need some kind of set variable saying which types of thing you currently care about. Or maybe the filter might need to be a complicated lambda function, who knows. It will depend on how much control you need to have over the filter.<p>One idea along these lines that I haven‚Äôt yet had the opportunity to try is to have a <em>lexically scoped</em> handler, installed for the duration of a particular block of code, and uninstalled again automatically by an object‚Äôs destructor. It might look something like this (details completely made up):<pre class=z-code><code><span class="z-text z-plain">function event_stream_consuming_coroutine() {
    // Here, we can receive any event type
    event = co_await EventQueue;

    {
        // Install a handler for a particular event we don‚Äôt want
        // to be bothered with in the next loop
        HandlerInstaller hi(EventQueue, EVENT_BORING, lambda(event) {
            fixed code to handle this event;
        });

        // Now do some kind of a loop over the remaining event
        // types, which can assume EVENT_BORING never appears.
        while (some condition) {
            // imagine lots of subsidiary control flow here
            event = co_await EventQueue;
        }

        // At the end of this block, the HandlerInstaller object
        // goes out of scope, and its handler is uninstalled
    }

    // And here we‚Äôre back to being able to receive any event,
    // even EVENT_BORING
    event = co_await EventQueue;
}
</span></code></pre><p>The idea is that the <code>HandlerInstaller</code> class has a constructor which arranges to add the specified event type to the pre-filtering system sitting in front of this coroutine. In this example, I‚Äôve also shown it taking a lambda function as an argument, so that we can say <em>what</em> to do with one of those <code>EVENT_BORING</code> events if it happens within this block.<p>Then, when the <code>HandlerInstaller</code> goes out of scope at the end of the block, its destructor runs, and uninstalls the handler again. So after the block ends, <code>EVENT_BORING</code> is no longer being filtered out of the coroutine‚Äôs input, and the final <code>co_await</code> could receive one.<p>I haven‚Äôt had a chance to experiment with this style yet, because you can‚Äôt do it with preprocessor coroutines. (Not even in C++, where you do have destructors. Preprocessor coroutines in C++ require all the coroutine‚Äôs persistent variables to be members of its containing class, so they can‚Äôt be scoped to sub-blocks of a function.) But you could do it in C++20‚Äôs native coroutines, where you get to declare block-local variables in a natural way. So when I write my first real system based on C++20 coroutines, I might give it a try!<h2 id=features>Coroutine paradigms<a aria-label="Anchor link for: features" class=zola-anchor href=#features style=visibility:hidden>#</a></h2><p>Finally, in this last section, I‚Äôll talk about some different ways to <em>think</em> about coroutines, in the sense of what kind of thing they even are.<p>I‚Äôve mentioned a couple of things of that kind in passing, in previous sections: I said that coroutines can be seen as the in-process analogue of a Unix pipe, or as the imperative-programming analogue of lazy evaluation (particularly something like Haskell‚Äôs lazy lists). But those were more about the kind of <em>tasks</em> you can use them for; the next few sections will be more about what they <em>are</em> than what you can use them as.<h3 id=taocp>TAOCP‚Äôs coroutines: symmetric, utterly stackless<a aria-label="Anchor link for: taocp" class=zola-anchor href=#taocp style=visibility:hidden>#</a></h3><p>Knuth‚Äôs presentation of coroutines in The Art of Computer Programming calls them a ‚Äògeneralisation‚Äô of subroutines. That characterisation doesn‚Äôt make sense in all contexts, but in that particular context, it does.<p>In TAOCP, all the example programs are written in machine code for the fictitious MIX architecture. MIX is strange by modern standards ‚Äì entire generations of CPU architecture fashion have come and gone <em>between</em> MIX and the more-or-less RISC architectures of today.<p>In particular, MIX has no real commitment to the existence of a <em>stack</em>. Typical CPU architectures of today consider a stack to be a fundamental necessity: there‚Äôs a special register reserved for use as the stack pointer, which it‚Äôs either impossible to use for any other purpose, or at least highly inadvisable. Too much of the design is based on the assumption that <em>of course</em> you‚Äôll want a stack.<p>But in MIX, nobody is making you use a stack. And Knuth, in many cases, doesn‚Äôt. His example MIX programs are generally set up so that each function has just one copy of its variables, stored at fixed addresses ‚Äì and when one function calls another, the return address is just another of those variables.<p>This way of organising code works fine for every purpose except recursion (and multithreading): if a function‚Äôs variables always live at the same address, it follows that you can‚Äôt have two independent instances of the function active at once. So a function may never call itself, either directly or through any other chain of intermediate functions. Or rather: if you <em>do</em> need a particular function to have multiple concurrent activations, you‚Äôll have to make a special effort to enable it to do so ‚Äì for example, by manually implementing a stack inside that function.<p>(It‚Äôs surprising to the modern eye how little emphasis TAOCP puts on recursion, in fact! Current treatments of programming tend to consider it to be one of the most fundamental and important techniques, to be taught early and used often. By contrast, Knuth clearly <em>knows</em> about it, but is able to get a startling amount done without having to resort to it.)<p>Anyway. The relevance of all this to coroutines is:<p>The MIX subroutine call instruction works like most RISC call instructions: rather than pushing the return address on to a stack in memory, it simply writes it into a particular register called rJ, and then it‚Äôs up to the callee to decide what to do with it. So a normal MIX function call works by the caller doing one of these jumps, and then the callee saves the rJ register in one of its own memory locations,<sup id=footnote-self-modifying-code>7</sup> so that it can remember where to jump back to at the end of its task.<p>The MIX call instruction is also its normal jump instruction. (rJ isn‚Äôt used for anything else, so that‚Äôs generally safe.) So the callee uses the same instruction when it returns, to transfer back to the caller. The only difference is that the caller jumps to the <em>start</em> of the callee, whereas the callee saves the rJ register on entry and jumps to wherever it said. On return, the caller <em>also</em> receives a value in rJ, because that jump instruction updated it too ‚Äì but it ignores it, because it doesn‚Äôt care which particular place in the callee control came back from.<p>And that‚Äôs the sense in which coroutines ‚Äògeneralise‚Äô subroutines. With subroutines, each side receives a value in rJ, but only one of them bothers to remember it. With coroutines written in MIX in this style, the only difference is that <em>both</em> sides remember the value they receive in rJ, and <em>both</em> sides use it as the place to transfer control to next.<p>So, unlike most languages‚Äô built-in coroutines (and certainly unlike my C preprocessor approach), this setup is completely symmetric. There is no sense in which one of the two functions ‚Äòreally is‚Äô the caller, and the other one is just ‚Äòpretending to be‚Äô the caller using a special kind of yield statement. Each one is doing exactly the same thing as the other. They are <em>co</em>-routines, in the sense of ‚Äúequal partners‚Äù.<h3 id=resumable-callee>A subroutine that can resume from where it last left off<a aria-label="Anchor link for: resumable-callee" class=zola-anchor href=#resumable-callee style=visibility:hidden>#</a></h3><p>Of course, in modern CPU architectures, there <em>is</em> a stack. And in almost all high-level languages (by which, in this case, I mean ‚Äòany higher than assembly‚Äô ‚Äì even COBOL counts), there will be a function call and return system that assumes a stack, and each function will store its local variables on the stack. So if we want to have functions interoperating in a coroutine style, and each of those functions occupies a region of memory on the call stack, then one function is going to <em>have</em> to be higher up the stack than the other.<p>This introduces an ugly asymmetry into the perfectly symmetric system Knuth described. We have to make an arbitrary decision about which way up to order the two functions on the stack, and whichever way round we make the decision, you might reasonably ask ‚ÄúWhy wouldn‚Äôt it be just as good to do it the other way?‚Äù<p>But it has to be done, so we make a choice, and designate one of our formerly equal coroutines as the ‚Äòcaller‚Äô and one as the ‚Äòcallee‚Äô. What are the consequences?<p>An invariant of the conventional stack-based function call system is that the currently executing function must be the deepest one on the stack. You just aren‚Äôt allowed to have extra stuff occupying stack space below<sup id=footnote-stack-grows-downwards>8</sup> the frame of the current function. (One reason is that asynchronous things like signal handlers or interrupts might overwrite that space without warning.)<p>So, when the caller is executing, <em>the callee can‚Äôt have a stack frame at all</em>. There would be nowhere for it to live.<p>Therefore, when the caller transfers control to the callee, the callee has to <em>construct</em> a stack frame. And when control transfers back the other way, that stack frame has to be thrown away again. So the physical view of what‚Äôs happening on the stack looks exactly like the caller making multiple independent calls to some subroutine: each time, a new stack frame is created, lives briefly, and is destroyed again.<p>The only differences between this and an ordinary subroutine is that the callee coroutine has to preserve all its variables between calls ‚Äì which means they have to be stored somewhere <em>other</em> than on the stack ‚Äì and that every time it‚Äôs ‚Äòcalled‚Äô, it has to resume from just after wherever it last returned from.<p>This is the compromise that most languages‚Äô coroutine systems end up settling on. My C preprocessor coroutine system is exactly this: a coroutine-structured function contains a macro at the start that can arrange to transfer control to any of the yield points in the function, and at each of those yield points, a variable is updated to mark that point as the one to resume from next time, and then the yield macro executes the ordinary C <code>return</code> statement. So the function‚Äôs physical stack frame is destroyed (which is OK because all the important state is kept in a <code>struct</code> elsewhere), and on the next call, a fresh stack frame is created.<p>The same is true of C++20 coroutines; it‚Äôs better hidden, but the low-level code generation will be doing basically this if you look deep enough. I expect the same is true of other languages that compile to ordinary stack-based native code.<p>This ‚Äòresumable subroutine‚Äô architecture has consequences beyond the details of the low-level implementation. Here are two:<p>Firstly, what happens if you want to divide up the work of the callee coroutine into subroutines? (For example, in Python, a generator can <code>yield from</code> another generator, so that the second one runs to completion and delivers all its results to the same place as the first generator.) The most obvious way to implement a ‚Äòsub-coroutine‚Äô is to have the parent coroutine call it repeatedly in a loop, passing its results back one by one:<pre class=z-code><code><span class="z-text z-plain">function parent_coroutine() {
    yield START;

    // Make an instance of a sub-coroutine
    coroutine_instance sub = sub_coroutine();
    // And run it to completion, passing on all its yields
    while (sub is not finished)
        yield sub.next_yielded_thing();

    yield END;
}
</span></code></pre><p>Now if even the <em>parent</em> coroutine is having its stack frame demolished completely on every yield, then the same thing must be happening to the sub-coroutine. So if we nest sub-coroutine ‚Äòcalls‚Äô <em>n</em> layers deep, then every time the innermost one yields and resumes, the implementation will have to tear down <em>n</em> stack frames and set them all up again. <em>The resumable-subroutine model makes deeply nested sub-coroutines expensive in performance.</em><p>There might be a way to get round this, one way or another. For example, in C++20 coroutines, you can <a rel="nofollow noreferrer" href=https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/coroutines-c++20/#stacked-generators>set up a promise class</a> in such a way that the logical ‚Äòstack‚Äô of sub-coroutines never occupies the physical stack all at once: instead, at each stage, a physical stack frame is only needed for the <em>currently executing</em> coroutine in the logical stack. But it‚Äôs not necessarily the <em>default</em> way that things will work, so it‚Äôs something you have to keep in mind and watch out for: if there‚Äôs a special way to avoid this stack usage problem, remember to do it if your nesting gets too deep, and if there isn‚Äôt one, don‚Äôt <em>let</em> your nesting get too deep.<p>Secondly, if your language supports exceptions, what happens if a coroutine throws one?<p>Exceptions propagate up the physical stack. So if the callee coroutine throws an exception, it can propagate out into the caller. (For example, an exception in a Python generator propagates out into the function that called <code>next()</code> on it.) But if the caller throws an exception, it can‚Äôt propagate in the reverse direction.<p>It may be possible to configure this to some extent. C++20 coroutine systems let you explicitly control what happens to an exception thrown in the coroutine. But you still can‚Äôt propagate an exception from ‚Äònormal‚Äô code <em>into</em> a coroutine by any sensible method.<p>So, if you care about exceptions, perhaps that might drive your choice of which of those two ‚Äòequal‚Äô coroutines to turn into the caller, and which the callee: in which direction (if any) would you like exceptions to travel between the two? Whichever direction that is, it might be easiest to make that ‚Äòup the stack‚Äô.<p>(And if, in some extra-confusing scenario, you want exceptions to be able to travel in <em>both</em> directions ‚Äì maybe based on the type of the exception ‚Äì then you <em>really</em> have a challenge on your hands. Good luck, and have fun.)<h3 id=cothreads>Cooperative threads that identify which thread to transfer to next<a aria-label="Anchor link for: cothreads" class=zola-anchor href=#cothreads style=visibility:hidden>#</a></h3><p>I performed a minor sleight of hand in the previous section. Did you spot it?<p>I said: if we want two functions to interoperate in coroutine style, and each one occupies a region of the stack, then one function has to be higher up the stack than the other.<p>But hang on. ‚ÄúThe‚Äù stack? <em>Who said there was only one?</em><p>This is the 2020s, and we‚Äôve met multithreading by now. So we‚Äôre already used to the idea that a complicated program might contain <em>multiple</em> stacks, in widely separated regions of memory. We could put our two interoperating coroutines on <em>separate</em> stacks.<p>This immediately solves the problem of sub-coroutines. Now each coroutine is free to be reorganised into as many subroutines as it likes, without impacting performance by having to forward values up and down a long chain, or having to constantly destroy and rebuild physical stack frames. Either stack can yield to the other, no matter how deep it‚Äôs currently nested; when it does, execution resumes in the other stack from wherever it left off, by restoring the <em>stack pointer</em> as well as the location within the current function.<p>Working in this style, it becomes natural for sub-coroutines to have a ‚Äòreturn value‚Äô, separate from whatever data they yielded. (For example, C++20 has <code>co_return</code>, separate from <code>co_yield</code>.) Just as an ordinary function performing I/O can return a value to its caller indicating success or failure, or returning the result of its I/O operation, a sub-coroutine that you call to yield on your behalf can return a value to you, perhaps communicating the same kind of thing.<p>Having two stacks restores the elegant symmetry of MIX‚Äôs coroutines: once again, our two (or more) stacks are equal partners in the program‚Äôs overall endeavour, with neither one being forced to take an artificially subordinate role.<p>But this is also what I meant when I said earlier that Knuth‚Äôs account of coroutines as ‚Äòa generalisation of subroutines‚Äô didn‚Äôt make sense in all contexts. In <em>this</em> model, coroutines and subroutines aren‚Äôt more and less general versions of the same concept: they‚Äôre <em>independent</em> concepts, and you have both of them at once. I like to imagine them at right angles to each other: coroutine transfers go left or right between stacks, whereas subroutine calls or returns go up and down within a stack.<p>OK. So: if each of our coroutines comes with its own stack, then doesn‚Äôt that make them a lot like threads? What‚Äôs the difference?<p>Firstly, these coroutine stacks are <em>cooperative</em>. Control only transfers between them when they ask for it on purpose. This makes them unlike ordinary threads, which are pre-emptively switched by the OS kernel, or (better still, if possible) actually run simultaneously on more than one CPU of your multi-core computer.<p>This cooperative nature makes it easy to avoid the synchronisation problems of real threads. You don‚Äôt need to take out a lock to stop another coroutine stack from accessing the same data as you: all you have to do is <em>not yield to it</em> while you‚Äôre in the middle of a critical section, and then it‚Äôs guaranteed not to be running at the time.<p>Cooperative multithreading isn‚Äôt totally unheard of even in normal OS contexts. In the late 1980s, for example, RISC OS came out, which was a GUI operating system that let you run multiple programs at once, but it was cooperatively multithreaded:<sup id=footnote-riscos>9</sup> your program would continue running without interruption until you called the ‚Äòwait for a GUI event‚Äô system service, and only then would the OS switch to running another process. And until <em>that</em> process made the same system call, <em>you‚Äôd</em> never get control back.<p>But when you did that, you didn‚Äôt know which other program would run next. It wasn‚Äôt really your business to know: the other programs were independent of you. If you exchanged data with another program at all, it would be mediated through the OS, perhaps via drag-and-drop or copy-and-paste, and you might not care <em>what</em> other program it was, only that it was providing data you had to do something with.<p>But in a coroutine-organised program, the data flow between the coroutines is highly structured, and the whole point. If you‚Äôre a coroutine stack acting as an adapter in between a producer and a consumer, then you most certainly do care which stack runs next: the consumer mustn‚Äôt run until you have a data item ready for it, and the producer mustn‚Äôt run until you‚Äôve disposed of the one you‚Äôre holding.<p>That‚Äôs the other vital difference. Coroutine stacks get to <em>identify which stack to switch to next.</em> That adapter thread doesn‚Äôt just ‚Äòyield to whoever has something to do‚Äô: it has two fundamentally different operations, ‚Äòyield to the producer‚Äô and ‚Äòyield to the consumer‚Äô.<p>So, that‚Äôs my third model of what kind of thing coroutines can be. They‚Äôre like cooperative threads, except that switching between them is done in a manner like a coroutine yield rather than like a system call: the yielding ‚Äòthread‚Äô chooses which ‚Äòthread‚Äô is going to run next, and maybe passes it a data object of some kind, which it might need in order to resume running at all.<p>I‚Äôve been talking about the CPU‚Äôs physical call stack structure throughout this section, but now I‚Äôll contradict myself by saying that, at the physical low level, depending on the language implementation, the ‚Äòstack‚Äô of each of these ‚Äòthreads‚Äô need not <em>actually</em> look anything like a physical call stack.<p>You <em>could</em> make it look like one. You could allocate a piece of memory for a coroutine stack; let the coroutines on it push and pop stack frames like ordinary function calls; and have a special ‚Äòyield‚Äô function that swaps out the stack pointer and switches over to executing on another stack. In fact, that‚Äôs not a bad way to add coroutines to a language that doesn‚Äôt already have them, because it doesn‚Äôt need the compiler to have any special knowledge of what‚Äôs going on. You could add coroutines to C in this way if you wanted to, and the approach would have several advantages over my preprocessor system. (Though also some disadvantages: this way, you have to anticipate every CPU architecture your code will need to run on, and write and test the low-level stack setup and switching code for all of them. Also, you lose the advantage I‚Äôm about to talk about in the <em>next</em> section.)<p>But it‚Äôs just as likely that a language‚Äôs implementation of coroutines will choose to do it another way. For example, in C++20, you can certainly arrange for coroutines to be able to call each other as subroutines, but the natural way to set it up involves the logical ‚Äòstack‚Äô of those calls being an array or a linked list inside the coroutines‚Äô promise objects. A separate physical call stack is never allocated, and (as I mentioned in the previous section) a call frame on the existing physical stack is only allocated for the currently running coroutine.<p>I‚Äôll end this section on an extra-confusing note: if the coroutine stacks are kept in a format completely separate from the physical call stack of a thread, that opens the possibility of combining stacks of coroutines <em>with</em> conventional threads ‚Äì so that when a coroutine stack has some work it can do, it could be resumed on the call stack of any thread that was free!<h3 id=named-object>A named object identifying a program activity<a aria-label="Anchor link for: named-object" class=zola-anchor href=#named-object style=visibility:hidden>#</a></h3><p>Finally, I want to go back to a point I touched on in <a href=https://shankun.github.io/posts/philosophy-of-coroutines/#vs-threads>a previous section</a>.<p>While I was comparing coroutines to ordinary threads (pre-emptive or genuinely concurrent), I mentioned that it‚Äôs useful that a suspended coroutine is always in a state where it can be conveniently destructed, if the computation or data stream or activity it‚Äôs producing is no longer needed.<p>More generally, once you‚Äôve started a coroutine and it‚Äôs suspended itself, there will typically be some kind of <em>object</em> in the programming language that represents its state. That object will behave like any other: you can assign it (or maybe a pointer to it) into a named variable, and you can store that variable wherever you like. You could have other objects contain pointers to activities they care about; you could keep all your activities in an array; you could store each one in a separate variable with an easy-to-remember name.<p>If your language supports automatically destructing things when they go out of scope, this lets you tie the lifetime of the coroutine to some other lifetime you already cared about. For example, if a particular other class is consuming the coroutine‚Äôs output, you can keep the coroutine object inside that class, and then you <em>just don‚Äôt need to worry</em> about clearing it up: the coroutine will automatically be thrown away exactly when nothing can possibly need its output any more, even if it was only half-finished. And if it had any internal variables that also needed cleaning up, that will all happen automatically too, because the language should arrange that <em>their</em> destructors run. In a language using RAII, even external resources like open files should be cleaned up with no fuss.<p>(If your language <em>doesn‚Äôt</em> support automatic destruction, you still get an echo of this usefulness. For example, if you‚Äôre using my preprocessor coroutine system in C, then each coroutine already has to have a structure type storing all its internal state. If you want to be able to abandon an unfinished coroutine, then you have to accompany the coroutine with a hand-written function that frees that state structure and any subsidiary resources its fields point to. But in C, you‚Äôre accustomed to writing those functions all the time anyway, so coroutines are no different ‚Äì and if a non-coroutine object of some kind owns a coroutine state, then the free function for the first object will naturally call the free function for the coroutine state. And <em>vice versa</em>. So it‚Äôs still natural to set up your code so that coroutines are cleanly destroyed; C makes you do all the <em>work</em> yourself, but it‚Äôs easy to <em>organise</em> it sensibly. This is one of the reasons I mentioned in the previous section for why I still use preprocessor coroutines in C instead of low-level stack switching: it would be a much harder job to free an in-progress <em>physical call stack</em> and all the resources allocated by unfinished calls on it!)<p>An interesting thing about this ‚Äòconveniently destructible‚Äô nature is that it wasn‚Äôt anywhere in TAOCP‚Äôs presentation of coroutines, and I presume therefore not in earlier work either. (That‚Äôs not <em>surprising</em>, of course, since languages with automatic destruction weren‚Äôt around in those days.) I only really thought about it that way myself when someone suggested to me the idea of combining coroutines with threads (as I mentioned at the end of the previous section), and wondered what the advantage might be over <em>just</em> using threads. I decided this was it: if your in-progress activities are explicitly stored in a data structure separate from the threads that are executing them, then you can abandon an activity in an easier and cleaner way than destroying a thread. It‚Äôs always interesting when the most useful feature of a thing turns out to be one that its originator never thought of!<p>Anyway. This is my final way of looking at coroutines: they allow you to encapsulate a computation, or some other particular strand of your program‚Äôs activity (such as one among many network connections it‚Äôs handling), into an object which has a name in your programming language, and can be accessed from outside the coroutine.<p>If you‚Äôve expanded each coroutine into a call stack of its own, as I discussed in the previous section, then this is still true: from outside the coroutine stack, you still have <em>one</em> named object, which encapsulates the whole stack. For example, if the coroutine yields data values, then that object will be where the rest of the program goes to get them: it will have the <code>next()</code> method, or the C++ iterable semantics, or whatever the language finds convenient. Whichever layer of the coroutine‚Äôs internal call stack yields a value, the values will all be delivered out of that same single object, without anyone outside the coroutine stack having to know or care about the internal subdivisions. And destroying that object should clean up the entire stack, even if it had multiple layers of unfinished internal subroutine calls.<p>But having a named object identifying an activity is useful for more things than just destroying it. Objects can also provide other methods, or expose their internal data to the rest of the program on purpose.<p>For example, if you have a coroutine object that encapsulates the progress of some activity, you might arrange that it presents methods returning details of its progress ‚Äì perhaps a headline figure like ‚Äò85% complete‚Äô for the graphical progress bar, or perhaps details of the last subdirectory it scanned, or what phase of a network connection it‚Äôs in. As usual, the advantage of doing this with coroutines rather than threads is that there‚Äôs no worry about concurrent access: if the coroutine is suspended, then you can safely read its data (or at least the data it‚Äôs publishing on purpose).<p>You might also arrange that methods on the wrapping object can <em>modify</em> the progress of the coroutine in some way. That case is harder to think of good examples for, but they do sometimes come up.<p>One example of this comes up in an example I‚Äôve already shown, from PuTTY. In <a href=https://shankun.github.io/posts/philosophy-of-coroutines/#protocol>a previous section</a> I showed pseudocode of the coroutine that handles the SSH transport layer, which periodically does key exchanges. A repeat key exchange can be triggered by either side sending the <code>KEXINIT</code> packet. In the pseudocode earlier I showed the coroutine responding to the <em>other</em> end sending <code>KEXINIT</code>. But what if <em>we‚Äôre</em> the side that decides it needs to rekey first?<p>Suppose we had a function <code>trigger_rekey()</code> that the rest of the program would call when it decided that needed to happen. That function could construct a <code>KEXINIT</code> packet and inject it into the output packet queue. When the other end responded with its own <code>KEXINIT</code>, the <code>key_exchange()</code> coroutine would resume, and see it. But then how would it know that in <em>this</em> case it shouldn‚Äôt respond by sending one of its own? Answer: the <code>trigger_rekey()</code> function also wants to set a flag <em>in the coroutine‚Äôs state</em> that says ‚ÄúWe‚Äôve already sent our <code>KEXINIT</code>, don‚Äôt send another one‚Äù. Then <code>key_exchange()</code> sees the incoming <code>KEXINIT</code> but also sees that the flag is set, so it goes back round to the top of the loop without sending an unwanted extra <code>KEXINIT</code> first. That‚Äôs the kind of thing that an accompanying mutator method on a coroutine might usefully do.<p>OK; what <em>else</em> can you do with a data object in a programming language, as well as deleting it, reading its state, or changing its state?<p>Here‚Äôs a <em>really</em> silly idea: you can <em>copy</em> it. Suppose you <em>cloned</em> a coroutine object, together with all of its internal state?<p>Using my C/C++ preprocessor coroutine system, this is perfectly possible. In that system, all the persistent variables of the coroutine ‚Äì including the state variable that says where to resume from next ‚Äì have to live in an explicitly declared structure (in C) or be members of a class (in C++). Either way, there‚Äôs no difficulty with making an exact copy: in C, you‚Äôd have to write a cloning function that dealt with memory allocation and deep copying, but that‚Äôs an easy mechanical job and the structure definition lists all the fields you need to handle. And in C++, you might very well be able to use the class‚Äôs default copy constructor and have it Just Work.<p>After you do that, you‚Äôve got two copies of the coroutine, and each of them will resume from the <em>same</em> part of the code when it next runs. It‚Äôs very like the Unix <code>fork</code> system call in that respect.<p>This isn‚Äôt a <em>deliberate</em> feature of my preprocessor system; it‚Äôs just a thing that drops out naturally from the implementation strategy, and turns out to be easy to do. I doubt that any language-native coroutine system will be willing to do this stunt.<p>But if you <em>can</em> do it, what might it be useful for?<p>One thing that springs to mind is testing, if your coroutine accepts input on each yield. Run the coroutine to a particular point, then repeatedly clone it and try lots of different inputs on independent copies of it, making sure that each one is handled correctly. This might be a lot less expensive that running from the start of the coroutine to the test point <em>again</em> for each test input.<p>Another possibility is to use the same idea at run time, to probe the coroutine from outside and <em>find</em> an input that will cause it to respond in a particular way. But I haven‚Äôt thought of a use for that at all!<h2 id=conclusion>Conclusion<a aria-label="Anchor link for: conclusion" class=zola-anchor href=#conclusion style=visibility:hidden>#</a></h2><p>I started writing this article because I wondered if I had anything in particular to say about coroutines. Turns out I had quite a lot!<p>Perhaps this will convince a few more people to become coroutine fans. Or perhaps it will give some existing coroutine users new ways of thinking about them and using them. Or perhaps not, and everyone reading this will decide I have weird opinions that they disagree with.<p>But it was fun to write, and helped me clarify my <em>own</em> thoughts. And it‚Äôs my blog so I can be self-indulgent if I want to. Thanks for reading!<h2 id=footnotes>Footnotes<a aria-label="Anchor link for: footnotes" class=zola-anchor href=#footnotes style=visibility:hidden>#</a></h2><p>With any luck, you should be able to read the footnotes of this article in place, by clicking on the superscript footnote number or the corresponding numbered tab on the right side of the page.<p>But just in case the CSS didn‚Äôt do the right thing, here‚Äôs the text of all the footnotes again:<p><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#footnote-win32-read-write>1.</a> For example, in the Win32 API, if you have a bidirectional file handle such as a serial port or a named pipe, and you need to simultaneously try to read and write it, I think it really <em>is</em> the least inconvenient thing to have two threads, one trying to read the handle and one trying to write it. The alternative <code>GetOverlappedResult</code> approach ended up being <em>more</em> painful when I tried it.<p><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#footnote-python-coroutines>2.</a> I talk about generators as if they were <em>the</em> coroutine system in Python, but in fact, they‚Äôre one of two: Python <em>also</em> has a <a rel="nofollow noreferrer" href=https://docs.python.org/3/reference/compound_stmts.html#coroutines>completely different kind of thing</a> that it actually uses the name ‚Äòcoroutine‚Äô for, in which function definitions start with <code>async def</code> and suspend with <code>await</code>. I‚Äôve never played with those, because generators are easier to get started with and have scratched my generalised coroutine itch well enough. But at some point I should at least give them a look.<p><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#footnote-knuth>3.</a> In TAOCP, one of Knuth‚Äôs suggestions for a producer/consumer chain is that you might be flexible enough to make a runtime decision as to <em>whether</em> to run both parts interleaved as coroutines, or whether to run them sequentially storing the intermediate data on disk, depending on how much memory you have. I suspect this was a more vital concern in the 1970s.<p><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#footnote-pairwise>4.</a> The non-cyclic version of this is also useful. As of Python 3.10 that <em>is</em> in the standard library, as <code>itertools.pairwise</code>, although I‚Äôm going to have to keep writing it by hand until I can rely on Python 3.10 or better being widespread enough. But as far as I know the cyclic version still isn‚Äôt in <code>itertools</code>.<p><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#footnote-StopIteration>5.</a> Incidentally, yes, I know, there‚Äôs another special case that my code doesn‚Äôt handle: if the input iterator has <em>no</em> values, then the example version of <code>cyclic_pairs</code> shown here will raise <code>StopIteration</code>, when perhaps you‚Äôd prefer it to silently generate an empty output list. Before Python 3.7 this happened automatically, because <code>StopIteration</code> was handled differently; in many of my own use cases I never pass an empty list anyway. But if you were writing a version general enough to go in <code>itertools</code>, you‚Äôd probably want to check for that case properly.<p><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#footnote-if-for-while>6.</a> In my fantasy programming language, this would <em>actually</em> be how the standard control structures were implemented. The only primitives in the language itself would be ‚Äògoto‚Äô and coroutines. <em>All</em> block-structured control flow commands, even the standard things like ‚Äòif‚Äô, ‚Äòfor‚Äô and ‚Äòwhile‚Äô, would be defined in the standard prelude as small coroutines of this kind, implemented in terms of ‚Äògoto‚Äô. Then they wouldn‚Äôt need to be keywords!<p><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#footnote-self-modifying-code>7.</a> In fact, Knuth does this by self-modifying code. The memory location where you save the return address is actually the destination-address field <em>within the jump instruction</em> that performs the return!<p><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#footnote-stack-grows-downwards>8.</a> For the sake of not using too many words, I‚Äôll assume the most common convention for stack layout, which is that the stack grows downwards in memory: pushes make SP smaller, pops make it bigger, and when you call a subroutine, its stack frame is below yours. If you‚Äôre used to one of the rare architectures that does things the other way up, just turn your head upside down while reading this section.<p><a href=https://shankun.github.io/posts/philosophy-of-coroutines/#footnote-riscos>9.</a> RISC OS is still around the last time I heard, and will run on a Raspberry Pi in particular. I have no idea if it‚Äôs <em>still</em> cooperatively multithreaded, though. Wikipedia seems to say it is, but its most recent citation is from 2003‚Ä¶</article><div class=giscus></div><script async crossorigin data-category=Announcements data-category-id=DIC_kwDOKg3Pks4CacrE data-emit-metadata=0 data-input-position=top data-lang=zh-CN data-loading=lazy data-mapping=pathname data-reactions-enabled=1 data-repo=shankun/shankun.github.io data-repo-id=R_kgDOKg3Pkg data-strict=0 data-theme=light src=https://giscus.app/client.js></script></div><footer><div class=copyright><p>¬© 2025 ShanKun</div><div class=credits>powered by <a rel="noreferrer noopener" href=https://www.getzola.org target=_blank>zola</a> and <a rel="noreferrer noopener" href=https://github.com/isunjn/serene target=_blank>serene</a></div></footer></main></div><script src=https://shankun.github.io/js/lightense.min.js></script><script src=https://shankun.github.io/js/main.js></script>